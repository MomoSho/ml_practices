{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5970: Machine Learning Practices__\n",
    "\n",
    "# Homework 11: Dimensionality Reduction\n",
    "\n",
    "## Assignment Overview\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "For all plots, make sure all necessary axes and curves are clearly and \n",
    "accurately labeled. Include figure/plot titles appropriately as well.\n",
    "Post any questions you have to the Canvas discussion.\n",
    "\n",
    "\n",
    "### Task\n",
    "For this assignment you will be exploring dimensionality reduction using\n",
    "Prinicipal Componenet Analysis (PCA). Having a large number of features \n",
    "can dramatically increase training times and the likelihood of overfitting.\n",
    "Additionally, it's difficult to visualize and understand patterns in high \n",
    "dimensional spaces. It's not uncommon that a lower dimensional subspace\n",
    "of the full feature space will better characterize trends within the data.\n",
    "PCA is one such technique that attempts to locate such subspaces and projects\n",
    "the data into the determined subspace.\n",
    "\n",
    "\n",
    "### Data set   \n",
    "The BMI data will be utilized. Recall:  \n",
    "* _MI_ files contain data with the number of activations for 48 neurons, at mutliple \n",
    "time points, for a single fold. There are 20 folds (20 files), where each fold consists \n",
    "of over 1000 times points (the rows). At each time point, we record the number of \n",
    "activations for each neuron for 20 bins. Therefore, each time point has 48 * 20 = 960 \n",
    "columns.   \n",
    "* _theta_ files record the angular position of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each time point.   \n",
    "* _dtheta_ files record the angular velocity of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each time point.   \n",
    "* _torque_ files record the torque of the shoulder (in column 0) and the elbow (in column \n",
    "1) for each time point.   \n",
    "* _time_ files record the actual time stamp of each time point.  \n",
    "\n",
    "\n",
    "### Objectives\n",
    "* Dimensionality Reduction\n",
    "* Principal Component Analysis (PCA)\n",
    "\n",
    "\n",
    "### Notes\n",
    "* Do not save work within the ml_practices folder\n",
    "\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [Sci-kit Learn Pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "* [Sci-kit Learn Preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)\n",
    "* [SciPy Paired t-test for Dependent Samples](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# THESE 3 IMPORTS ARE CUSTOM .py FILES AND CAN BE FOUND \n",
    "# ON THE SERVER AND GIT\n",
    "import visualize\n",
    "import metrics_plots\n",
    "from pipeline_components import DataSampleDropper, DataFrameSelector\n",
    "from pipeline_components import DataScaler, DataLabelEncoder\n",
    "\n",
    "from KFoldHolisticCrossValidation import KFoldHolisticCrossValidation, generate_paramsets\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools\n",
    "import time as timelib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as peffects\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, roc_curve, auc, f1_score\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "FIGWIDTH = 5\n",
    "FIGHEIGHT = 5\n",
    "FONTSIZE = 10\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (FIGWIDTH, FIGHEIGHT)\n",
    "plt.rcParams['font.size'] = FONTSIZE\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = FONTSIZE\n",
    "plt.rcParams['ytick.labelsize'] = FONTSIZE\n",
    "\n",
    "%matplotlib inline\n",
    "#https://matplotlib.org/3.1.1/tutorials/introductory/images.html\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display current working directory of this notebook. If you are using \n",
    "relative paths for your data, then it needs to be relative to the CWD.\n",
    "\"\"\"\n",
    "HOME_DIR = pathlib.Path.home()\n",
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED \"\"\"\n",
    "def read_bmi_file_set(directory, filebase):\n",
    "    '''\n",
    "    Read a set of CSV files and append them together\n",
    "    :param directory: The directory in which to scan for the CSV files\n",
    "    :param filebase: A file specification that potentially includes wildcards\n",
    "    :returns: A list of Numpy arrays (one for each fold)\n",
    "    '''\n",
    "    \n",
    "    # The set of files in the directory\n",
    "    files = fnmatch.filter(os.listdir(directory), filebase)\n",
    "    files.sort()\n",
    "\n",
    "    # Create a list of Pandas objects; each from a file in the directory that matches filebase\n",
    "    lst = [pd.read_csv(directory + \"/\" + file, delim_whitespace=True).values for file in files]\n",
    "    \n",
    "    # Concatenate the Pandas objects together.  ignore_index is critical here so that\n",
    "    # the duplicate row indices are addressed\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Load the BMI data from all the folds\n",
    "From the MI_folds we will predict torque_folds\n",
    "\"\"\"\n",
    "# TODO: set path appropriately\n",
    "dir_name = str(HOME_DIR / 'ml_practices/imports/datasets/bmi/DAT6_08')\n",
    "MI_folds = read_bmi_file_set(dir_name, 'MI_fold*') \n",
    "theta_folds = read_bmi_file_set(dir_name, 'theta_fold*')\n",
    "dtheta_folds = read_bmi_file_set(dir_name, 'dtheta_fold*')\n",
    "torque_folds = read_bmi_file_set(dir_name, 'torque_fold*')\n",
    "time_folds = read_bmi_file_set(dir_name, 'time_fold*')\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Print out the shape of all the data for each fold\n",
    "\"\"\"\n",
    "# Zip all data together for convenience when looping\n",
    "alldata_folds = zip(MI_folds, theta_folds, dtheta_folds, \n",
    "                    torque_folds, time_folds)\n",
    "for i, (MI, theta, dtheta, torque, time) in enumerate(alldata_folds):\n",
    "    print(\"FOLD %2d \" % i, MI.shape, theta.shape, \n",
    "          dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Summary statistics\n",
    "\"\"\"\n",
    "print(\"Means\")\n",
    "all_MI = np.concatenate(MI_folds, axis=0)\n",
    "all_theta = np.concatenate(theta_folds, axis=0)\n",
    "all_dtheta = np.concatenate(dtheta_folds, axis=0)\n",
    "all_torque = np.concatenate(torque_folds, axis=0)\n",
    "all_time = np.concatenate(time_folds, axis=0)\n",
    "\n",
    "df = np.concatenate(([all_MI.mean()], np.mean(all_theta, axis=0), np.mean(all_dtheta, axis=0), \n",
    "                     np.mean(all_torque, axis=0))).reshape(1,-1)\n",
    "df = pd.DataFrame(df, columns=['MI', 'Should. angle', 'Elbow angle', \n",
    "                               'Should. d_angle', 'Elbow d_angle', \n",
    "                               'Should. torque', 'Elbow torque'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION\n",
    "From the MI_folds we will predict torque_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Evaluate the training performance of an already trained model\n",
    "\"\"\"\n",
    "def compute_rmse(x, y):\n",
    "    return np.sqrt(np.nanmean((x - y)**2))\n",
    "\n",
    "def predict_score_rmse(model, X, y):\n",
    "    '''\n",
    "    Compute the model predictions and cooresponding scores.\n",
    "    PARAMS:\n",
    "        X: feature data\n",
    "        y: corresponding output\n",
    "    RETURNS:\n",
    "        rmse: root mean squared error\n",
    "        score: score computed by the models score() method\n",
    "        preds: predictions of the model from X\n",
    "    '''\n",
    "    preds = model.predict(X)\n",
    "    score = model.score(X, y)\n",
    "    rmse = compute_rmse(y, preds)\n",
    "    return rmse, score, preds\n",
    "\n",
    "def predict_plot(model, X, y, time, titles, xlims=None):\n",
    "    '''\n",
    "    Compute the model's predicted output\n",
    "    PARAMS:\n",
    "        model: already trained model\n",
    "        X: inputs\n",
    "        y: outputs \n",
    "        * For plots\n",
    "        time: time axis of timestamps\n",
    "        titles: subplot titles for each output column\n",
    "        xlims: two element list of the x limits for the plot\n",
    "    '''\n",
    "    # Compute and evaulate  predictions on the model\n",
    "    rmse, score, preds = predict_score_rmse(model, X, y)\n",
    "    print(\"RMSE: %.3f\" % rmse)\n",
    "    print(\"R^2: %.3f\" % score)\n",
    "\n",
    "    noutputs = y.shape[1]\n",
    "    \n",
    "    # Construct the plots\n",
    "    fig, axs = plt.subplots(noutputs,1, figsize=(25,4))\n",
    "    fig.subplots_adjust(hspace=.5)\n",
    "    axs = axs.ravel()\n",
    "    for i, ax in enumerate(axs):\n",
    "        ax.plot(time, preds[:,i], 'r', label='Prediction')\n",
    "        ax.plot(time, y[:,i], 'b', label='True')\n",
    "        ax.set(title=titles[i], ylabel=r'$\\tau$ (N/m)')\n",
    "        ax.set(xlim=xlims)\n",
    "    axs[-1].set(xlabel='Time (s)')\n",
    "    axs[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain the first 1 folds (i.e. index 0)\n",
    "\n",
    "Split the data into X (i.e. the MI_folds) and y (i.e. the torque_folds).\n",
    "\n",
    "Hold out a subset of the data, before training and cross validation\n",
    "\"\"\"\n",
    "# List of the output cloumn names\n",
    "output_names = ['Shoulder', 'Elbow']\n",
    "\n",
    "# TODO: Grab the first fold\n",
    "Xtrain = # TODO\n",
    "ytrain = # TODO\n",
    "time_trn = # TODO\n",
    "\n",
    "# TODO: Obtain 2nd to last fold for validation\n",
    "Xval = # TODO\n",
    "yval = # TODO\n",
    "time_val = # TODO\n",
    "\n",
    "# TODO: Obtain last fold for testing\n",
    "Xtest = # TODO\n",
    "ytest = # TODO\n",
    "time_test = # TODO\n",
    "\n",
    "nfeatures = Xtrain.shape[1]\n",
    "\n",
    "Xtrain.shape, ytrain.shape, Xval.shape, yval.shape, Xtest.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BENCHMARK\n",
    "The task is to predict shoulder and elbow torque from the neural activations.\n",
    "We are going to compare the performance of the LinearRegression model trained on the original data to the LinearRegression model trained on the PCA transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegresson Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "LinearRegression benchmark for comparision\n",
    "\"\"\"\n",
    "benchmark_lnr = LinearRegression()\n",
    "benchmark_lnr.fit(Xtrain, ytrain)\n",
    "\n",
    "# Compute predictions on fully trained model for train set\n",
    "predict_plot(benchmark_lnr, Xtrain, ytrain, time_trn, \n",
    "             output_names, xlims=[50,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions on fully trained model for val set\n",
    "predict_plot(benchmark_lnr, Xval, yval, time_val, \n",
    "             output_names, xlims=[2675,2725])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create a PCA object and fit it on the training set with whiten=True\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Get an idea of the number of PCs neccessary to represent the data\n",
    "Use pca.explained_variance_ratio to get a fraction for each \n",
    "corresponding PC, and np.cumsum() to get the cumuluative sums as \n",
    "each component is successively considered.\n",
    "\"\"\"\n",
    "# TODO: Compute the cumulative fraction of explained variance\n",
    "explained = # TODO\n",
    "\n",
    "# Plot the cumulative fraction of explained variance\n",
    "plt.figure(figsize=(FIGWIDTH*3,FIGHEIGHT))\n",
    "plt.plot(explained)\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Fraction of Variance Explained')\n",
    "plt.title('Cumulative Fraction of Variance Explained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain the minimum number of PCs necessary to account for 95% of \n",
    "the total variance. You can use np.where to locate the indices in \n",
    "the cumulative sum that is greater than or equal to .95, and then \n",
    "add 1 to the list of indices returned to get the number of PCs.\n",
    "The first element in the list is the minimum number of PCs to\n",
    "account for 95% of the variance.\n",
    "\"\"\"\n",
    "majority_explained = # TODO\n",
    "\n",
    "# Display the determined number of PCs\n",
    "nPCs = majority_explained[0]\n",
    "nPCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Using the number of PCs obtained above, re-fit the PCA with\n",
    "whiten=True and project the training data into PC space\n",
    "\"\"\"\n",
    "pca = # TODO\n",
    "pca.fit(Xtrain)\n",
    "\n",
    "# TODO: Project into PC-space\n",
    "Xtrain_pca = # TODO\n",
    "Xtrain_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project back into the original space\n",
    "Xtrain_recon = # TODO\n",
    "Xtrain_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the reconstruction error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Implement a model Pipeline. The first step of the pipeline is \n",
    "PCA with n_components set to the number of PCs determined above\n",
    "and whiten to true; and the second step of the pipeline is\n",
    "LinearRegression()\n",
    "\"\"\"\n",
    "# TODO: Create Pipeline model\n",
    "pca_model = Pipeline([\n",
    "    ('PCA', # TODO), \n",
    "    ('Regression', # TODO)\n",
    "])\n",
    "\n",
    "# TODO: Fit model to entire train set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predictions on fully trained model for train set\n",
    "# Display the plot of the true output overlaying the predicted output\n",
    "# You can use predict_plot() with xlims=[50,100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predictions on fully trained model for val set\n",
    "# Display the plot of the true output overlaying the predicted output\n",
    "# You can use predict_plot() with xlims=[2675,2725]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRIDSEARCH KFoldHolisticCrossValidation\n",
    "Use the KFoldHolisticCrossValidation from the HW 11 folder to show training and validation set performance as a function of data set size. The hyper-parameter you should vary for PCA is n_components. Briefly discuss and interepret the results of the GridSearch in terms of train size, performance, and variations in the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Evaulation function for KFoldHolisticCrossValidation\n",
    "\"\"\"\n",
    "def mse_rmse(trues, preds):\n",
    "    '''\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse_rads = np.sqrt(mse)\n",
    "    rmse_degs = rmse_rads * 180 / np.pi\n",
    "    return mse, rmse_rads, rmse_degs\n",
    "\n",
    "def score_eval(model, X, y, preds):\n",
    "    '''\n",
    "    Compute the model predictions and corresponding scores, for an\n",
    "    already trained model.\n",
    "    PARAMS:\n",
    "        model: model to predict with\n",
    "        X: input feature data\n",
    "        y: true output for X\n",
    "        preds: predicted output for X\n",
    "    RETURNS: results as a dictionary of numpy arrays\n",
    "        mse: mean squared error for each column\n",
    "        rmse_rads: rMSE in radians\n",
    "        rmse_deg: rMSE in degrees\n",
    "        evar: explained variance, best is 1.0\n",
    "        score: score computed by the models score() method\n",
    "    '''\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    mse, rmse_rads, rmse_degs = mse_rmse(y, preds)\n",
    "    evar = explained_variance_score(y, preds)\n",
    "    \n",
    "    # Dictionary of numpy arrays. The numpy arrays must\n",
    "    # be row vectors, where each element is the result \n",
    "    # for a different output, when using multiple regression.\n",
    "    # The keys of the dictionary are the name of the performance \n",
    "    # metric, and the values are the numpy row vectors\n",
    "    results = {'mse': np.reshape(mse, (1, -1)), \n",
    "               'rmse_rads': np.reshape(rmse_rads, (1, -1)), \n",
    "               'rmse_degs': np.reshape(rmse_degs, (1, -1)), \n",
    "               'evar': np.reshape(evar, (1, -1)), \n",
    "               'score': np.reshape(score, (1, -1)), \n",
    "              }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of number of PCs to try\n",
    "components = np.append(np.logspace(0, 5, num=6, base=3, dtype=int), nPCs)\n",
    "components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Create the KFoldHolisticCrossValidation object using the PCA \n",
    "pipeline model created above\n",
    "Estimated runtime <140min on mlserver\n",
    "\"\"\"\n",
    "# Grid Search Parameters\n",
    "opt_metric = 'rmse_degs'\n",
    "maximize_opt_metric = False\n",
    "trainsizes = range(1, 11)\n",
    "rotation_skip = 1\n",
    "\n",
    "# TODO: GridSearch pipeline hyper-parameters can be specified \n",
    "# with ‘__’ separated parameter names\n",
    "hyperparam_grid = {\n",
    "    'PCA__n_components': # TODO: set to the components list created in the cell above  \n",
    "    'PCA__whiten': [True]\n",
    "}\n",
    "hyperparams = generate_paramsets(hyperparam_grid)\n",
    "nhyperparams = len(hyperparams)\n",
    "\n",
    "\n",
    "# TODO: Save Parameters. Set these appropriately\n",
    "force = False \n",
    "write_crossval = False\n",
    "fullcvfname = \"hw11_crossval_%02dparams.pkl\" % nhyperparams\n",
    "\n",
    "\n",
    "if force or (not os.path.exists(fullcvfname)):\n",
    "    # TODO: Create the cross validation object\n",
    "    crossval = # TODO\n",
    "\n",
    "    t0 = timelib.time()\n",
    "    # TODO: Execute cross validation for all parameters and sizes\n",
    "    \n",
    "    \n",
    "    # TODO: Save the cross validation object. Can use joblib.dump()\n",
    "    if write_crossval: # TODO\n",
    "    \n",
    "    lapsedTime = timelib.time() - t0\n",
    "    print(\" ** Elapsed Time %.2f min\" % (lapsedTime / 60))\n",
    "\n",
    "else:\n",
    "    # TODO: Load the cross val object from file. Can use joblib.load()\n",
    "    crossval = # TODO\n",
    "    \n",
    "crossval.model, crossval.rotation_skip, crossval.trainsizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the lists of the best parameter sets for each size \n",
    "from the cross validation using get_report_best_params_all_sizes\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display available metrics\n",
    "\"\"\"\n",
    "crossval.results[0]['results']['val'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display available summary (mean and std) metrics\n",
    "\"\"\"\n",
    "crossval.results[0]['summary']['val'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the validation results for all parameter sets over all train \n",
    "sizes, for the specified metrics, rmse_degs_mean and evar_mean\n",
    "(this variable is declared above). Use plot_allparams_val()\n",
    "\"\"\"\n",
    "metrics = ['rmse_degs_mean', 'evar_mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "For the best parameter set for the train set size at \n",
    "size_idx=0 (i.e. 1 fold), plot just the TRAIN and VAL set performances using \n",
    "plot_param_train_val() for just the opt_metric\n",
    "\"\"\"\n",
    "size_idx = 0\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "bp_idx = # TODO: obtain the best parameter index for the size\n",
    "# TODO: call plot_param_train_val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Re-fit PCA model with best hyper-parameters for train size of \n",
    "3 folds\n",
    "\"\"\"\n",
    "print(\"Train size %d folds\" % trainsizes[0])\n",
    "\n",
    "bp_idx = crossval.best_param_inds[0]\n",
    "best_params = crossval.paramsets[bp_idx]\n",
    "\n",
    "# Set the hyperparameters of the Pipeline model\n",
    "pca_model.set_params(**best_params)\n",
    "\n",
    "# Fit the model to entire train set\n",
    "pca_model.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predictions on fully trained model for train set\n",
    "# Display the plot of the true output overlaying the predicted output\n",
    "# You can use predict_plot() with xlims=[50,100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute predictions on fully trained model for val set\n",
    "# Display the plot of the true output overlaying the predicted output\n",
    "# You can use predict_plot() with xlims=[2675,2725]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION\n",
    "1. Bake off. Compare the training and validation performances of the benchmark linear model learned without PCA to the model learned using PCA for train size of 1 fold. Based on the validation performances, which would you choose and why?\n",
    "\n",
    "2. Now that you've selected your model, observe and compare the test results. Was your selection justified? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set these paths appropriately\n",
    "# Re-load saved favorite crossval object\n",
    "r_crossval = joblib.load('../hw07/hw7_full_ridge_crossval.pkl')\n",
    "# Re-load saved linear crossval object\n",
    "lnr_crossval = joblib.load('../hw07/hw7_full_linear_crossval.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display Linear Regression model performance\n",
    "\"\"\"\n",
    "lnr_crossval.plot_param_train_val([lnr_crossval.opt_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: any additional plots or tables relevant to your discussion responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
