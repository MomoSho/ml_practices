{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5970: Machine Learning Practices__\n",
    "\n",
    "# Homework 6: Cross Validation\n",
    "\n",
    "## Assignment Overview\n",
    "First read through the entire notebook, do not write any code. This assignment\n",
    "is more complex than previous, and it will be helpful to have a sense of \n",
    "the structure before you start coding.  \n",
    "\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "All the plotting functions have been provided. You should not need to alter\n",
    "any of these.\n",
    "\n",
    "### Task\n",
    "For this assignment you will be implementing __holistic cross validation__. \n",
    "Cross validation is a procedure that involves training, validating, and testing \n",
    "a model on different subsets of the data set to evaluate how well the model will \n",
    "generalize to unseen examples. Additionally, cross validation is a good tool \n",
    "for evaulating models when only small amounts of data are available.  \n",
    "\n",
    "The train sets are utilized for the various models to learn with, the validation \n",
    "sets are utilized to initially evaluate and select the best performing model. \n",
    "The test sets are utilized to determine how well the choosen model actually\n",
    "will generalize to unseen examples.  \n",
    "\n",
    "The validation and test sets can often seem similar conceptually, however, the\n",
    "key difference is that the validation performance is used to actually make \n",
    "guided decisions about model tuning (i.e., hyper-parameter values). Decisions \n",
    "about which hyper-parameters to use are never done based on the test set. The \n",
    "test set performance evaluates the generalized performance on data unused for \n",
    "hyper-parameter selection and training.\n",
    "\n",
    "### Data set\n",
    "The BMI data will be utilized. Recall: \n",
    "* _MI_ files contain data with the number of activations for 48 neurons, at mutliple \n",
    "time points, for a single fold. There are 20 folds (20 files), where each fold consists \n",
    "of over 1000 times points (the rows). At each time point, we record the number of \n",
    "activations for each neuron for 20 bins. Therefore, each time point has 48 * 20 = 960 \n",
    "columns.  \n",
    "* _theta_ files record the angular position of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each time point.  \n",
    "* _dtheta_ files record the angular velocity of the shoulder (in column 0) and the elbow \n",
    "(in column 1) for each time point.  \n",
    "* _torque_ files record the torque of the shoulder (in column 0) and the elbow (in column \n",
    "1) for each time point.  \n",
    "* _time_ files record the actual time stamp of each time point.  \n",
    "\n",
    "\n",
    "### Objectives\n",
    "* Implement and understand __holistic cross validation__\n",
    "* Training set size sensitivity analysis\n",
    "\n",
    "### Notes\n",
    "* Do not save work within the ml_practices folder\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools, time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "FIGW = 10\n",
    "FIGH = 6\n",
    "FONTSIZE = 12\n",
    "\n",
    "HOME_DIR = pathlib.Path.home()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (FIGW, FIGH)\n",
    "plt.rcParams['font.size'] = FONTSIZE\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = FONTSIZE\n",
    "plt.rcParams['ytick.labelsize'] = FONTSIZE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display current working directory of this notebook. If you are using relative \n",
    "paths for your data, then it needs to be relative to the CWD.\n",
    "\"\"\"\n",
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bmi_file_set(directory, filebase):\n",
    "    '''\n",
    "    Read a set of CSV files and append them together\n",
    "    :param directory: The directory in which to scan for the CSV files\n",
    "    :param filebase: A file specification that potentially includes wildcards\n",
    "    :returns: A list of Numpy arrays (one for each fold)\n",
    "    '''\n",
    "    \n",
    "    # The set of files in the directory\n",
    "    files = fnmatch.filter(os.listdir(directory), filebase)\n",
    "    files.sort()\n",
    "\n",
    "    # Create a list of Pandas objects; each from a file in the directory that matches filebase\n",
    "    lst = [pd.read_csv(directory + \"/\" + file, delim_whitespace=True).values for file in files]\n",
    "    \n",
    "    # Concatenate the Pandas objects together.  ignore_index is critical here so that\n",
    "    # the duplicate row indices are addressed\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Load the BMI data from all the folds, using read_bmi_file_set()\n",
    "\"\"\"\n",
    "# TODO: might need to change; assumes ml_practices is in home directory\n",
    "dir_name = str(HOME_DIR / 'ml_practices/imports/datasets/bmi/DAT6_08')\n",
    "\n",
    "MI_folds = read_bmi_file_set(dir_name, 'MI_fold*')\n",
    "theta_folds = read_bmi_file_set(dir_name, 'theta_fold*')\n",
    "dtheta_folds = read_bmi_file_set(dir_name, 'dtheta_fold*')\n",
    "torque_folds = read_bmi_file_set(dir_name, 'torque_fold*')\n",
    "time_folds = read_bmi_file_set(dir_name, 'time_fold*')\n",
    "\n",
    "alldata_folds = zip(MI_folds, theta_folds, dtheta_folds, \n",
    "                    torque_folds, time_folds)\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Print out the shape of all the data for each fold\n",
    "\"\"\"\n",
    "for i, (MI, theta, dtheta, torque, time) in enumerate(alldata_folds):\n",
    "    print(\"FOLD %2d \" % i, MI.shape, theta.shape, \n",
    "          dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Construct the Cartesian product of the parameters\n",
    "\"\"\"\n",
    "def generate_paramsets(param_lists):\n",
    "    '''\n",
    "    Construct the Cartesian product of the parameters\n",
    "    PARAMS:\n",
    "        params_lists: dict of lists of values to try for each parameter.\n",
    "                      keys of the dict are the names of the parameters\n",
    "                      values are lists of values to try for the  \n",
    "                      corresponding parameter\n",
    "    RETURNS: a list of dicts that make up the Cartesian product of the \n",
    "             parameters\n",
    "    '''\n",
    "    keys, values = zip(*param_lists.items())\n",
    "    # Determines cartesian product of parameter values\n",
    "    combos = itertools.product(*values)\n",
    "    # Constructs list of dictionaries\n",
    "    combos_dicts = [dict(zip(keys, vals)) for vals in combos]\n",
    "    return list(combos_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE EVALUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Evaluate the performance of an already trained model on some data\n",
    "\"\"\"\n",
    "def mse_rmse(trues, preds):\n",
    "    '''\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse_rads = np.sqrt(mse)\n",
    "    rmse_degs = rmse_rads * 180 / np.pi\n",
    "    return mse, rmse_rads, rmse_degs\n",
    "\n",
    "\"\"\" TODO\n",
    "Finish implementation by just returning the dictionary of results\n",
    "\"\"\"\n",
    "def score_eval(model, X, y, preds):\n",
    "    '''\n",
    "    Compute the model predictions and corresponding scores, for an\n",
    "    already trained model.\n",
    "    PARAMS:\n",
    "        model: model to predict with\n",
    "        X: input feature data\n",
    "        y: true output for X\n",
    "        preds: predicted output for X\n",
    "    RETURNS: results as a dictionary of numpy arrays\n",
    "        mse: mean squared error for each column\n",
    "        rmse_rads: rMSE in radians\n",
    "        rmse_deg: rMSE in degrees\n",
    "        evar: explained variance, best is 1.0\n",
    "        score: score computed by the models score() method\n",
    "    '''\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    mse, rmse_rads, rmse_degs = mse_rmse(y, preds)\n",
    "    evar = explained_variance_score(y, preds)\n",
    "    \n",
    "    # TODO: Complete the results dictionary. This is a \n",
    "    # dictionary of numpy arrays. The numpy arrays must\n",
    "    # be row vectors, where each element is the result \n",
    "    # for a different output, when using multiple regression.\n",
    "    # The keys of the dictionary are the name of the performance \n",
    "    # metric, and the values are the numpy row vectors\n",
    "    results = {'mse': np.reshape(mse, (1, -1)), \n",
    "               'rmse_rads': # TODO\n",
    "               'rmse_degs': # TODO\n",
    "               'evar': # TODO\n",
    "               'score': # TODO\n",
    "              }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Complete KFoldHolisticCrossValidation implementation\n",
    "General Procedure:\n",
    "+ iter over hyper-parameter sets\n",
    "  1. set hyper-parameters of the model\n",
    "  2. iter over train set sizes\n",
    "     a. iter over data set splits/rotations\n",
    "          i. train the model\n",
    "         ii. evaluate the model on train, val, and test sets\n",
    "        iii. record the results\n",
    "     b. record the results by size\n",
    "  3. record the results by hyper-parameter set\n",
    "\"\"\"\n",
    "class KFoldHolisticCrossValidation():\n",
    "    def __init__(self, model, paramsets, eval_func, opt_metric, \n",
    "                 maximize_opt_metric=False, trainsizes=[1], rotation_skip=1):\n",
    "        ''' TODO\n",
    "        Object for managing and performing cross validation for a given model for \n",
    "        a list of parameter sets and train set sizes. Note, train set size is in \n",
    "        terms of number of folds (not samples)\n",
    "        PARAMS:\n",
    "            model: base ML model\n",
    "            \n",
    "            paramsets: list of dicts of parameter sets to give to the model\n",
    "            \n",
    "            eval_func: handle to function used to evaluate/score the model\n",
    "                       The eval_func must have the following arguments: model, \n",
    "                       X, ytrue, ypreds and return a dict of numpy arrays with \n",
    "                       shape 1-by-n, where n is the number of outputs if using \n",
    "                       multiple regression.\n",
    "                       template function header: eval_func(model, X, y, preds)\n",
    "                       template output: {'metrics1':1_by_n_array, ...}\n",
    "                       \n",
    "            opt_metric: the optized metric. one of the metric key names  \n",
    "                        returned from eval_func to use to pick the best \n",
    "                        parameter sets\n",
    "                        \n",
    "            maximize_opt_metric: True if opt_metric is maximized; False if minimized\n",
    "            \n",
    "            trainsizes: list of training set sizes (in number of folds) to try\n",
    "            \n",
    "            rotation_skip: build model and evaluate every ith rotation (1=all \n",
    "                           possible rotations; 2=every other rotation, etc.)\n",
    "        ''' \n",
    "        # TODO: set the class variables\n",
    "        self.model = #TODO\n",
    "        self.paramsets = #TODO\n",
    "        self.trainsizes = #TODO\n",
    "        self.eval_func = #TODO\n",
    "        self.opt_metric = opt_metric + '_mean'\n",
    "        self.maximize_opt_metric = maximize_opt_metric\n",
    "        self.rotation_skip = rotation_skip\n",
    "        \n",
    "        # Results attributes\n",
    "        # Full recording of all results for all paramsets, sizes, rotations, \n",
    "        # and metrics. This is a list of dictionaries for each paramset\n",
    "        self.results = None\n",
    "        # Validation summary report of all means and standard deviations for \n",
    "        # all metrics, for all paramsets, and sizes. This is a 3D s-by-r-by-p \n",
    "        # numpy array. Where s is the number of sizes, r the number of summary \n",
    "        # metrics +2, and p is the number of paramsets\n",
    "        self.report_by_size = None\n",
    "        # List of the indices of the best paramset for each size\n",
    "        self.best_param_inds = None\n",
    "\n",
    "    def perform_cross_validation(self, all_Xfolds, all_yfolds, trainsize, verbose=0):\n",
    "        ''' TODO: This is where the bulk of the work will be done\n",
    "        Perform cross validation for a singular train set size and single hyper-parameter \n",
    "        set, by evaluating the model's performance over multiple data set rotations all\n",
    "        of the same size.\n",
    "\n",
    "        NOTE: This function assumes the hyper-parameters have already been set in the model\n",
    "            \n",
    "        PARAMS:\n",
    "            all_Xfolds: list containing all of the input data folds\n",
    "            all_yfolds: list containing all of the output data folds\n",
    "            trainsize: number of folds to use for training\n",
    "            verbose: flag to display simple debugging information\n",
    "            \n",
    "        RETURNS: train, val, and test set results for all rotations of the data sets and \n",
    "                 the summary (i.e. the averages over all the rotations) of the results.\n",
    "                 results is a dictionary of dictionaries of r-by-n numpy arrays. Where r\n",
    "                 is the number of rotations, and n is the number of outputs from the model.\n",
    "                 summary is a dictionary of dictionaries of 1-by-n numpy arrays. \n",
    "\n",
    "                 General form:\n",
    "                     results.keys() = ['train', 'val', 'test']\n",
    "\n",
    "                     results['train'].keys() = ['metric1', 'metric2', ...]\n",
    "                     \n",
    "                     results['train']['metric1'] = numpy_array\n",
    "                     \n",
    "                     results = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse'      : r_by_n_numpy_array,\n",
    "                                     'rmse_rads': r_by_n_numpy_array, \n",
    "                                     'rmse_degs': r_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "                     \n",
    "                     summary = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse_mean'      : 1_by_n_numpy_array,\n",
    "                                     'mse_std'       : 1_by_n_numpy_array,\n",
    "                                     'rmse_rads_mean': 1_by_n_numpy_array, \n",
    "                                     'rmse_rads_std' : 1_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "\n",
    "                    For example, you can access the MSE results for the validation\n",
    "                    set like so:\n",
    "                        results['train'][metric] \n",
    "                    For example, you can access the summary (i.e. the average results \n",
    "                    over all the rotations) for the test set for the rMSE in degrees \n",
    "                    like so:\n",
    "                        summary['test']['rmse_degs_mean']                \n",
    "        '''\n",
    "        \n",
    "        # Verify a valid train set size was provided\n",
    "        nfolds = len(all_Xfolds)\n",
    "        if trainsize > nfolds - 2: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.perform_cross_validation() - \"\n",
    "            err_msg += \"trainsize (%d) cant be more than nfolds (%d) - 2\" % (trainsize, nfolds)\n",
    "            raise ValueError(err_msg)\n",
    "        \n",
    "        # Set up results recording for each rotation\n",
    "        results = {'train': None, 'val': None, 'test': None}\n",
    "        summary = {'train': {}, 'val': {}, 'test': {}}\n",
    "        \n",
    "        model = self.model\n",
    "        evaluate = self.eval_func\n",
    "        \n",
    "        # TODO: Rotate through the data to try different train, val, and test sets\n",
    "        for rotation in range(0, nfolds, self.rotation_skip):\n",
    "            # TODO: Determine fold indices for train, val, and test set. \n",
    "            #       The val and tests are each only 1 fold\n",
    "            trainfolds = # TODO\n",
    "            valfold = # TODO\n",
    "            testfold = # TODO\n",
    "        \n",
    "            # TODO: Construct train set by concatenating the individual training \n",
    "            #       folds together (hint: see np.take() and np.concatenate())\n",
    "            X = # TODO\n",
    "            y = # TODO\n",
    "\n",
    "            # TODO: Construct validation set. Hint: this is always one fold\n",
    "            Xval = # TODO\n",
    "            yval = # TODO\n",
    "            \n",
    "            # TODO: Construct test set\n",
    "            Xtest = # TODO\n",
    "            ytest = # TODO\n",
    "            \n",
    "            # DEBUGGING\n",
    "            if verbose:\n",
    "                print(\"TRAIN\", X.shape, y.shape, trainfolds)\n",
    "                print(\"VAL\", Xval.shape, yval.shape, valfold)\n",
    "                print(\"TEST\", Xtest.shape, ytest.shape, testfold)\n",
    "            \n",
    "            # TODO: Train model using the training set\n",
    "\n",
    "            \n",
    "            # TODO: Predict with the model for train, val, and test sets\n",
    "            preds = #TODO\n",
    "            preds_val = #TODO\n",
    "            preds_test = #TODO\n",
    "            \n",
    "            # TODO: Evaluate the model for each set\n",
    "            res_train = #TODO\n",
    "            res_val = #TODO\n",
    "            res_test = #TODO\n",
    "\n",
    "            # Record the train, val, and test set results. These are dicts \n",
    "            # of result metrics, returned by the evaluate function\n",
    "            # TODO: For the first rotation, store the results from evaluating\n",
    "            #       with the train, val, and tests by setting the values of   \n",
    "            #       the appropriate items within the results dict\n",
    "            if results['train'] is None: \n",
    "                results['train'] = #TODO\n",
    "                results['val'] = #TODO\n",
    "                results['test'] = #TODO\n",
    "            else:\n",
    "                # Append the results for each rotation\n",
    "                for metric in res_train.keys():\n",
    "                    results['train'][metric] = np.append(results['train'][metric], \n",
    "                                                         res_train[metric], axis=0)\n",
    "                    results['val'][metric] = np.append(results['val'][metric], \n",
    "                                                       res_val[metric], axis=0)\n",
    "                    results['test'][metric] = np.append(results['test'][metric], \n",
    "                                                        res_test[metric], axis=0)\n",
    "\n",
    "        # Compute and record the mean and standard deviation for the given size for each metric\n",
    "        for metric in results['train'].keys():\n",
    "            for stat_set in ['train', 'val', 'test']:\n",
    "                summary[stat_set][metric+'_mean'] = np.mean(results[stat_set][metric], \n",
    "                                                            axis=0).reshape(1, -1)\n",
    "                summary[stat_set][metric+'_std'] = np.std(results[stat_set][metric], \n",
    "                                                          axis=0).reshape(1, -1)\n",
    "\n",
    "        return results, summary\n",
    "\n",
    "    def grid_cross_validation(self, all_Xfolds, all_yfolds, verbose=0):\n",
    "        ''' TODO\n",
    "        (MAIN PROCEDURE) Perform cross validation for multiple sets of \n",
    "        parameters and train set sizes. Calls self.perform_cross_validation(). \n",
    "        This is the procedure that executes cross validation for all parameter \n",
    "        sets and all sizes.\n",
    "        \n",
    "        PARAMS:\n",
    "            all_Xfolds: all the input data folds (list of folds, as it was  \n",
    "                        loaded from the files)\n",
    "            all_yfolds: all the output data folds (list of folds)\n",
    "            verbose: flag to print out simple debugging information\n",
    "            \n",
    "        RETURNS: best parameter set for each train set size as a list of \n",
    "                 parameter indices. Additionally, returns self.report_by_size,\n",
    "                 the 3D array of validation means (overall rotations) for all \n",
    "                 paramsets, for each metric, for all sizes. The structure of \n",
    "                 the returned object is a dictionary of the following form: \n",
    "                 { \n",
    "                   'report_by_size' : self.report_by_size, \n",
    "                   'best_param_inds': self.best_param_inds\n",
    "                 }\n",
    "        ''' \n",
    "        sizes = self.trainsizes\n",
    "        paramsets = self.paramsets\n",
    "        nparamsets = len(paramsets)\n",
    "        print(\"nparamsets\", nparamsets)\n",
    "        \n",
    "        # Set up all results\n",
    "        all_results = []\n",
    "        \n",
    "        # Iterate over parameter sets\n",
    "        for params in paramsets:\n",
    "            # Set up paramset results \n",
    "            param_res = []\n",
    "            param_smry = None\n",
    "            \n",
    "            # Set model parameters\n",
    "            print(\"Current paramset\\n\", params)\n",
    "            self.model.set_params(**params)\n",
    "\n",
    "            # Iterate over the different train set sizes\n",
    "            for size in sizes:\n",
    "                # TODO: Cross-validation for current model and train size\n",
    "                res, smry = # TODO\n",
    "\n",
    "                # Save the results\n",
    "                param_res.append(res) \n",
    "                # Save the mean and standard deviation statistics (summary)\n",
    "                if param_smry is None: param_smry = smry\n",
    "                else:\n",
    "                    # For each metric measured, append the summary results\n",
    "                    for metric in smry['train'].keys():\n",
    "                        for stat_set in ['train', 'val', 'test']:\n",
    "                            stat = smry[stat_set][metric]\n",
    "                            param_smry[stat_set][metric] = np.append(param_smry[stat_set][metric],\n",
    "                                                                     stat, axis=0)\n",
    "            \n",
    "            # Append the results and summary for the parameter set\n",
    "            all_results.append({'params':params, 'results':param_res, \n",
    "                                'summary':param_smry})\n",
    "        \n",
    "        # Generate reports and determine best params for each size \n",
    "        self.results = all_results\n",
    "        self.report_by_size = self.get_reports()\n",
    "        self.best_param_inds = self.get_best_params(self.opt_metric, \n",
    "                                                    self.maximize_opt_metric)\n",
    "        return {'report_by_size':self.report_by_size, \n",
    "                'best_param_inds':self.best_param_inds}\n",
    "\n",
    "    def get_reports(self):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary of all the parameters for each size\n",
    "        for all metrics. This is used to determine the best parameter set  \n",
    "        for each size\n",
    "        \n",
    "        RETURNS: the report_by_size as a 3D s-by-r-by-p array. Where s is \n",
    "                 the number of train sizes tried, r is the number of summary  \n",
    "                 metrics evaluated+2, and p is the number of parameter sets.\n",
    "        '''\n",
    "        results = self.results\n",
    "        sizes = np.reshape(self.trainsizes, (1, -1))\n",
    "        \n",
    "        nsizes = sizes.shape[1]\n",
    "        nparams = len(results)\n",
    "        \n",
    "        # Set up the reports objects\n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics \n",
    "        report_by_size = np.empty((nsizes, len(colnames), nparams), dtype=object)\n",
    "\n",
    "        # Determine mean val for each paramset for each size for all metrics\n",
    "        for p, paramset_result in enumerate(results):\n",
    "            params = paramset_result['params']\n",
    "            res_val = paramset_result['summary']['val']\n",
    "\n",
    "            # Compute mean val result for each train size for each metric\n",
    "            means_by_size = [np.mean(res_val[metric], axis=1) for metric in metrics]\n",
    "            # Include the train set sizes into the report\n",
    "            means_by_size = np.append(sizes, means_by_size, axis=0)\n",
    "            # Include the parameter sets into the report\n",
    "            param_strgs = np.reshape([str(params)]*nsizes, (1, -1))\n",
    "            means_by_size = np.append(param_strgs, means_by_size, axis=0).T\n",
    "            # Append the parameter set means into the report \n",
    "            report_by_size[:,:,p] = means_by_size\n",
    "        return report_by_size\n",
    "\n",
    "    def get_best_params(self, opt_metric, maximize_opt_metric):\n",
    "        ''' PROVIDED (Do read through all the provided code)\n",
    "        Determines the best parameter set for each train size, based \n",
    "        on a specific metric.\n",
    "        \n",
    "        PARAMS:\n",
    "            opt_metric: optimized metric. one of the metrics returned \n",
    "                        from eval_func, with '_mean' appended for the\n",
    "                        summary stat. This is the mean metric used to  \n",
    "                        determine the best parameter set for each size\n",
    "                        \n",
    "            maximize_opt_metric: True if the max of opt_metric should be\n",
    "                                 used to determine the best parameters.\n",
    "                                 False if the min should be used.\n",
    "        RETURNS: list of best parameter set indicies for each size \n",
    "        '''\n",
    "        results = self.results\n",
    "        report_by_size = self.report_by_size \n",
    "                \n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        \n",
    "        # Determine best params for each size, for the optimized metric\n",
    "        best_param_inds = None\n",
    "        metric_idx = metrics.index(opt_metric)\n",
    "        \n",
    "        if maximize_opt_metric:\n",
    "            # Add two for the additional cols for params and size\n",
    "            best_param_inds = np.argmax(report_by_size[:, metric_idx+2, :], axis=1)\n",
    "        else: \n",
    "            best_param_inds = np.argmin(report_by_size[:, metric_idx+2, :], axis=1)\n",
    "        # Return list of best params indices for each size\n",
    "        return best_param_inds\n",
    "    \n",
    "    def get_best_params_strings(self):\n",
    "        ''' PROVIDED\n",
    "        Generates a list of strings of the best params for each size\n",
    "        RETURNS: list of strings of the best params for each size\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        results = self.results\n",
    "        return [str(results[p]['params']) for p in best_param_inds]\n",
    "\n",
    "    def get_report_best_params_for_size(self, size):\n",
    "        ''' PROVIDED\n",
    "        Get the mean validation summary for the best parameter set \n",
    "        for a specific size for all metrics.\n",
    "        PARAMS:\n",
    "            size: index of desired train set size for the best  \n",
    "                  paramset to come from. Size here is the index in \n",
    "                  the trainsizes list, NOT the actual number of folds.\n",
    "        RETURNS: the best parameter report for the size as an s-by-m  \n",
    "                 dataframe. Where each row is for a different size, and \n",
    "                 each column is for a different summary metric.\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        report_by_size = self.report_by_size \n",
    "        \n",
    "        bp_index = best_param_inds[size]\n",
    "                \n",
    "        metrics = list(self.results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics\n",
    "        report_best_params_for_size = pd.DataFrame(report_by_size[:,:,bp_index],\n",
    "                                                   columns=colnames)\n",
    "        return report_best_params_for_size\n",
    "\n",
    "    def plot_cv(self, foldsindices, results, summary, metrics, size):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after perform_cross_validation(), \n",
    "        displaying the train and val set performances for each rotation \n",
    "        of the training set. \n",
    "        \n",
    "        PARAMS:\n",
    "            foldsindices: indices of the train sets tried\n",
    "            results: results from perform_cross_validation()\n",
    "            summary: mean and standard deviations of the results\n",
    "            metrics: list of result metrics to plot. Available metrics \n",
    "                     are the keys in the dict returned by eval_func\n",
    "            size: train set size\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(results['train'][metric], axis=1)\n",
    "            res_val = np.mean(results['val'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(foldsindices, res_train, label='train')\n",
    "            ax.plot(foldsindices, res_val, label='val')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[0].legend(loc='upper right')\n",
    "        axs[0].set(xlabel='Fold Index')\n",
    "        axs[0].set(title='Performance for Train Set Size ' + str(size))\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_param_train_val(self, metrics, paramidx=0, view_test=False):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), \n",
    "        displaying the mean (summary) train and val set performances \n",
    "        for each train set size.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std'\n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            paramidx: parameter set index\n",
    "            view_test: flag to view the test set results\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "\n",
    "        summary = results[paramidx]['summary']\n",
    "        params = results[paramidx]['params']\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(summary['train'][metric], axis=1)\n",
    "            res_val = np.mean(summary['val'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(sizes, res_train, label='train')\n",
    "            ax.plot(sizes, res_val, label='val')\n",
    "            if view_test:\n",
    "                res_test = np.mean(summary['test'][metric], axis=1)\n",
    "                ax.plot(sizes, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title=str(params))\n",
    "        axs[0].legend(loc='upper right')\n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_allparams_val(self, metrics):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying  \n",
    "        mean (summary) validation set performances for each train size \n",
    "        for all parameter sets for the specified metrics.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std' \n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(10,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            for p, param_results in enumerate(results):\n",
    "                summary = param_results['summary']\n",
    "                params = param_results['params']\n",
    "                # Compute the mean for multiple outputs\n",
    "                res_val = np.mean(summary['val'][metric], axis=1)                \n",
    "                ax.plot(sizes, res_val, label=str(params))\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title='Validation Performance')\n",
    "        axs[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_best_params_by_size(self):\n",
    "        ''' PROVIDED\n",
    "        Plotting function for after grid_cross_validation(), displaying \n",
    "        mean (summary) train and validation set performances for the best \n",
    "        parameter set for each train size for the specified metrics.\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        results = self.results\n",
    "        metric = self.opt_metric\n",
    "        best_param_inds = self.best_param_inds\n",
    "        sizes = np.array(self.trainsizes)\n",
    "\n",
    "        # Unique set of best params for the legend\n",
    "        unique_param_sets = np.unique(best_param_inds)\n",
    "        lgnd_params = [self.paramsets[p] for p in unique_param_sets]\n",
    "\n",
    "        # Initialize figure\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10,6))\n",
    "        fig.subplots_adjust(hspace=.4)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "        set_names = ['train', 'val']\n",
    "\n",
    "        # Construct each subplot\n",
    "        for i, (ax, set_name) in enumerate(zip(axs, set_names)):\n",
    "            for p in unique_param_sets:\n",
    "                # Obtain indices of sizes this paramset was best for\n",
    "                param_size_inds = np.where(best_param_inds == p)[0]\n",
    "                param_sizes = sizes[param_size_inds]\n",
    "                # Compute the mean over multiple outputs for each size\n",
    "                param_summary = results[p]['summary'][set_name]\n",
    "                metric_scores = np.mean(param_summary[metric][param_size_inds, :], axis=1)\n",
    "                # Plot the param results for each size it was the best for\n",
    "                ax.scatter(param_sizes, metric_scores, s=120, marker=(p+2, 1))\n",
    "                #ax.grid(True)\n",
    "\n",
    "            set_name += ' Set Performance'\n",
    "            ax.set(ylabel=metric, title=set_name)\n",
    "\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].legend(lgnd_params, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM CROSS VALIDATION FOR ELASTICNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Generate list of parameters to use for cross validation\n",
    "using generate_paramsets()\n",
    "\"\"\"\n",
    "param_lists = {'alpha':[.001, .005, .01, .05, .1], \n",
    "               'l1_ratio':[.05, .1], 'max_iter':[1e4]}\n",
    "\n",
    "allparamsets = # TODO\n",
    "allparamsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Initialize the cross validation object. Use ElasticNet for the \n",
    "ase model, use every even value between 2 and 18, inclusive, for \n",
    "the train set sizes, use score_eval as the eval_func, use rmse_degs \n",
    "as the metric to optimize, and 4 for the skip. We want ot minimize \n",
    "rmse thus set maximize_opt_metrix=False\n",
    "\"\"\"\n",
    "model = # TODO\n",
    "trainsizes = # TODO\n",
    "opt_metric = # TODO\n",
    "maximize_opt_metric = # TODO\n",
    "skip = # TODO\n",
    "crossval = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Execute the grid_cross_validation() procedure for all parameters \n",
    "and train set sizes\n",
    "\"\"\"\n",
    "# TODO: make sure this is set appropriately. True if you want to \n",
    "#       just always to run cross validation, false if you want \n",
    "#       to re-load a previous run\n",
    "force = True \n",
    "fullcvfname = \"hw6_crossval.pkl\"\n",
    "\n",
    "crossval_report = None\n",
    "if force or (not os.path.exists(fullcvfname)):\n",
    "    # TODO: Use grid_cross_validation() to run the full cross \n",
    "    #       validation procedure\n",
    "    # Note: when testing, run this using small lists of parameters \n",
    "    #       (e.g. of length 2 or 4) and/or small trainsize lists \n",
    "    #       (e.g. [1, 2, 3, 4, 5])\n",
    "    # Note: for the final submission, make sure to use the complete \n",
    "    #       parameter set list and trainsize list provided/specified\n",
    "    #       This will take some time.\n",
    "    crossval_report = #TODO\n",
    "    joblib.dump(crossval, fullcvfname)\n",
    "else:\n",
    "    # TODO: Re-load saved crossval object instead of re-running the\n",
    "    #       cross validation procedure. Use joblib.load()\n",
    "    crossval = # TODO\n",
    "    crossval_report = {'report_by_size' : crossval.report_by_size, \n",
    "                       'best_param_inds': crossval.best_param_inds}\n",
    "\n",
    "crossval_report.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain all the results for all parameters, for all sizes, for all\n",
    "rotations. This is the results attribute of the crossval object \n",
    "\"\"\"\n",
    "all_results = # TODO\n",
    "len(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the keys of the results object\n",
    "\"\"\"\n",
    "all_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain and display the indices of the best parameters for each \n",
    "size using either the best_params_inds attribute of the crossval\n",
    "object or 'best_param_inds' item from the crossval_report dict\n",
    "\"\"\"\n",
    "best_param_inds = # TODO\n",
    "best_param_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Display the list of the best parameter sets for each size. Use\n",
    "crossval.get_best_params_strings()\n",
    "\"\"\"\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Obtain and dsplay the shape of the report of all the parameters'   \n",
    "mean results over all sizes and rotations. This is the report_by_size \n",
    "attribute of the crossval object. It is also stored within the \n",
    "'report_by_size' item of the crossval_report dict\n",
    "\"\"\"\n",
    "report = # TODO\n",
    "report.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized \n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the average results (summary) over train set size for all \n",
    "parameter sets for the metrics 'rmse_degs_mean' and 'evar_mean'\n",
    "for the train and val sets. Use plot_param_train_val(). \n",
    "view_test=False\n",
    "\"\"\"\n",
    "metrics = ['rmse_degs_mean', 'evar_mean']\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the validation results for all parameters over all train \n",
    "sizes, for the specified metrics. Use plot_allparams_val()\n",
    "\"\"\"\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "For the best parameter set for the train set size at index 5,\n",
    "plot the TRAIN, VAL, and TEST set performances using \n",
    "plot_param_train_val() for just the optimized metric\n",
    "\"\"\"\n",
    "size_idx = 5\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Use get_report_best_params_for_size() to display the report of  \n",
    "the average val statistics for the best parameter set, for the \n",
    "train set size at index 5 (i.e. size_idx)\n",
    "\"\"\"\n",
    "report_best_params = # TODO\n",
    "report_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "257px",
    "width": "365px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
