{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NAME:__ __FULLNAME__  \n",
    "__SECTION:__ __NUMBER__  \n",
    "__CS 5970: Machine Learning Practices__\n",
    "\n",
    "# Homework 7: Model Comparisons\n",
    "\n",
    "## Assignment Overview\n",
    "Generally, it's helpful to first read through the entire notebook before writing\n",
    "any code to obtain a sense of the overall program structure before you start coding.  \n",
    "\n",
    "Follow the TODOs and read through and understand any provided code.  \n",
    "\n",
    "\n",
    "### Task\n",
    "For this assignment, you'll be comparing different models after performing holistic\n",
    "cross validation to find the best parameter sets for various sizes of the training\n",
    "data.\n",
    "\n",
    "\n",
    "### Data set\n",
    "The BMI data will be utilized. Recall:  \n",
    "* _MI_ files contain data with the number of activations for 48 neurons, at mutliple time points, for a single fold. There are 20 folds (20 files), where each fold consists of over 1000 times points (the rows). At each time point, we record the number of activations for each neuron for 20 bins. Therefore, each time point has 48 * 20 = 960 columns.  \n",
    "* _theta_ files record the angular position of the shoulder (in column 0) and the elbow (in column 1) for each time point. \n",
    "* _dtheta_ files record the angular velocity of the shoulder (in column 0) and the elbow (in column 1) for each time point.  \n",
    "* _torque_ files record the torque of the shoulder (in column 0) and the elbow (in column 1) for each time point. \n",
    "* _time_ files record the actual time stamp of each time point.  \n",
    "\n",
    "\n",
    "### Objectives\n",
    "* Understanding regularization using __holistic cross validation__\n",
    "* Training set size sensitivity analysis\n",
    "* Model selection\n",
    "\n",
    "### Notes\n",
    "* Do not save work within the ml_practices folder\n",
    "\n",
    "### General References\n",
    "* [Guide to Jupyter](https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook)\n",
    "* [Python Built-in Functions](https://docs.python.org/3/library/functions.html)\n",
    "* [Python Data Structures](https://docs.python.org/3/tutorial/datastructures.html)\n",
    "* [Numpy Reference](https://docs.scipy.org/doc/numpy/reference/index.html)\n",
    "* [Numpy Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Numpy_Python_Cheat_Sheet.pdf)\n",
    "* [Summary of matplotlib](https://matplotlib.org/3.1.1/api/pyplot_summary.html)\n",
    "* [DataCamp: Matplotlib](https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264365&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=9026223&gclid=CjwKCAjw_uDsBRAMEiwAaFiHa8xhgCsO9wVcuZPGjAyVGTitb_-fxYtkBLkQ4E_GjSCZFVCqYCGkphoCjucQAvD_BwE)\n",
    "* [Pandas DataFrames](https://urldefense.proofpoint.com/v2/url?u=https-3A__pandas.pydata.org_pandas-2Ddocs_stable_reference_api_pandas.DataFrame.html&d=DwMD-g&c=qKdtBuuu6dQK9MsRUVJ2DPXW6oayO8fu4TfEHS8sGNk&r=9ngmsG8rSmDSS-O0b_V0gP-nN_33Vr52qbY3KXuDY5k&m=mcOOc8D0knaNNmmnTEo_F_WmT4j6_nUSL_yoPmGlLWQ&s=h7hQjqucR7tZyfZXxnoy3iitIr32YlrqiFyPATkW3lw&e=)\n",
    "* [Sci-kit Learn Linear Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n",
    "* [Sci-kit Learn Ensemble Models](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n",
    "* [Sci-kit Learn Metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Sci-kit Learn Model Selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "* [SciPy Paired t-test for Dependent Samples](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html)\n",
    "* [Student's t-test](https://en.wikipedia.org/wiki/Student%27s_t-test#Dependent_t-test_for_paired_samples)\n",
    "* [Understanding Paired t-tests](https://machinelearningmastery.com/how-to-code-the-students-t-test-from-scratch-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os, re, fnmatch\n",
    "import pathlib, itertools, time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "FIGW = 10\n",
    "FIGH = 6\n",
    "FONTSIZE = 12\n",
    "\n",
    "HOME_DIR = pathlib.Path.home()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (FIGW, FIGH)\n",
    "plt.rcParams['font.size'] = FONTSIZE\n",
    "\n",
    "plt.rcParams['xtick.labelsize'] = FONTSIZE\n",
    "plt.rcParams['ytick.labelsize'] = FONTSIZE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display current working directory of this notebook. If you are using \n",
    "relative paths for your data, then it needs to be relative to the CWD.\n",
    "\"\"\"\n",
    "pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bmi_file_set(directory, filebase):\n",
    "    '''\n",
    "    Read a set of CSV files and append them together\n",
    "    :param directory: The directory in which to scan for the CSV files\n",
    "    :param filebase: File specification potentially including wildcards\n",
    "    :returns: A list of Numpy arrays (one for each fold)\n",
    "    '''\n",
    "    \n",
    "    # The set of files in the directory\n",
    "    files = fnmatch.filter(os.listdir(directory), filebase)\n",
    "    files.sort()\n",
    "\n",
    "    # Create list of Pandas objects; \n",
    "    # Each from a file in the directory matching the filebase\n",
    "    lst = [pd.read_csv(directory + \"/\" + file, delim_whitespace=True).values \n",
    "           for file in files]\n",
    "    \n",
    "    # Concatenate the Pandas objects together. ignore_index is \n",
    "    # critical here so that the duplicate row indices are addressed\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Load the BMI data from all the folds, using read_bmi_file_set()\n",
    "\"\"\"\n",
    "# TODO: might need to change directory\n",
    "dir_name = str(HOME_DIR / 'ml_practices/imports/datasets/bmi/DAT6_08')\n",
    "\n",
    "MI_folds = read_bmi_file_set(dir_name, 'MI_fold*')\n",
    "theta_folds = read_bmi_file_set(dir_name, 'theta_fold*')\n",
    "dtheta_folds = read_bmi_file_set(dir_name, 'dtheta_fold*')\n",
    "torque_folds = read_bmi_file_set(dir_name, 'torque_fold*')\n",
    "time_folds = read_bmi_file_set(dir_name, 'time_fold*')\n",
    "\n",
    "alldata_folds = zip(MI_folds, theta_folds, dtheta_folds, \n",
    "                    torque_folds, time_folds)\n",
    "\n",
    "nfolds = len(MI_folds)\n",
    "nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Print out the shape of all the data for each fold\n",
    "\"\"\"\n",
    "for i, (MI, theta, dtheta, torque, time) in enumerate(alldata_folds):\n",
    "    print(\"FOLD %2d \" % i, MI.shape, theta.shape, \n",
    "          dtheta.shape, torque.shape, time.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETER SET LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paramsets(param_lists):\n",
    "    '''\n",
    "    Construct the Cartesian product of the parameters\n",
    "    PARAMS:\n",
    "        params_lists: dict of lists of values to try for each parameter.\n",
    "                      keys of the dict are the names of the parameters\n",
    "                      values are lists of values to try for the \n",
    "                      corresponding parameter\n",
    "    RETURNS: a list of dicts that make up the Cartesian product of the \n",
    "    parameters\n",
    "    '''\n",
    "    keys, values = zip(*param_lists.items())\n",
    "    # Determines cartesian product of parameter values\n",
    "    combos = itertools.product(*values)\n",
    "    # Constructs list of dictionaries\n",
    "    combos_dicts = [dict(zip(keys, vals)) for vals in combos]\n",
    "    return list(combos_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE EVALUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_rmse(trues, preds):\n",
    "    '''\n",
    "    Compute MSE and rMSE for each column separately.\n",
    "    '''\n",
    "    mse = np.sum(np.square(trues - preds), axis=0) / trues.shape[0]\n",
    "    rmse_rads = np.sqrt(mse)\n",
    "    rmse_degs = rmse_rads * 180 / np.pi\n",
    "    return mse, rmse_rads, rmse_degs\n",
    "\n",
    "def score_eval(model, X, y, preds):\n",
    "    '''\n",
    "    Compute the model predictions and corresponding scores, for an\n",
    "    already trained model.\n",
    "    PARAMS:\n",
    "        model: model to predict with\n",
    "        X: input feature data\n",
    "        y: true output for X\n",
    "        preds: predicted output for X\n",
    "    RETURNS: results as a dictionary of numpy arrays\n",
    "        mse: mean squared error for each column\n",
    "        rmse_rads: rMSE in radians\n",
    "        rmse_deg: rMSE in degrees\n",
    "        evar: explained variance, best is 1.0\n",
    "        score: score computed by the models score() method\n",
    "    '''\n",
    "    score = model.score(X, y)\n",
    "\n",
    "    mse, rmse_rads, rmse_degs = mse_rmse(y, preds)\n",
    "    evar = explained_variance_score(y, preds)\n",
    "    \n",
    "    # Dictionary of numpy arrays. The numpy arrays must\n",
    "    # be row vectors, where each element is the result \n",
    "    # for a different output, when using multiple regression.\n",
    "    # The keys of the dictionary are the name of the performance \n",
    "    # metric, and the values are the numpy row vectors\n",
    "    results = {'mse': np.reshape(mse, (1, -1)), \n",
    "               'rmse_rads': np.reshape(rmse_rads, (1, -1)), \n",
    "               'rmse_degs': np.reshape(rmse_degs, (1, -1)), \n",
    "               'evar': np.reshape(evar, (1, -1)), \n",
    "               'score': np.reshape(score, (1, -1)), \n",
    "              }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO: \n",
    "FILL IN WITH YOUR SOLUTION FROM HW6 for perform_cross_validation(). All \n",
    "that needs to be done here is simply copy/paste your code from HW6 into\n",
    "perform_cross_validation()\n",
    "\"\"\"\n",
    "class KFoldHolisticCrossValidation():\n",
    "    def __init__(self, model, paramsets, eval_func, opt_metric, \n",
    "                 maximize_opt_metric=False, trainsizes=[1], rotation_skip=1):\n",
    "        ''' \n",
    "        Object for managing and performing cross validation for a given \n",
    "        model for a list of parameter sets and train set sizes. Note, \n",
    "        train set size is in terms of number of folds (not samples)\n",
    "        \n",
    "        General Procedure:\n",
    "        + iter over hyper-parameter sets\n",
    "          1. set hyper-parameters of the model\n",
    "          2. iter over train set sizes\n",
    "             a. iter over splits/rotations\n",
    "                  i. train the model\n",
    "                 ii. evaluate the model on train, val, and test sets\n",
    "                iii. record the results\n",
    "             b. record the results by size\n",
    "          3. record the results by hyper-parameter set\n",
    "\n",
    "        PARAMS:\n",
    "            model: base ML model\n",
    "            \n",
    "            paramsets: list of dicts of parameter sets to give to the model\n",
    "            \n",
    "            eval_func: handle to function used to evaluate/score the model\n",
    "                       The eval_func definition must have the following  \n",
    "                       arguments: model, X, ytrue, ypreds; and return a dict \n",
    "                       of numpy arrays with shape 1-by-n, where n is the\n",
    "                       number of outputs if using multiple regression.\n",
    "                       template function header: \n",
    "                           def eval_func(model, X, y, preds)\n",
    "                       template output: \n",
    "                           {'metrics1':1_by_n_array, ...}\n",
    "                       \n",
    "            opt_metric: the optimized metric. one of the metric key names \n",
    "                        returned from eval_func to use to pick the best \n",
    "                        parameter sets\n",
    "                        \n",
    "            maximize_opt_metric: True if opt_metric is maximized; \n",
    "                                 False if minimized\n",
    "            \n",
    "            trainsizes: list of training set sizes (in number of folds) to try\n",
    "            \n",
    "            rotation_skip: build model and evaluate every ith rotation (1=all \n",
    "                           possible rotations; 2=every other rotation, etc.)\n",
    "        ''' \n",
    "        self.model = model\n",
    "        self.paramsets = paramsets\n",
    "        self.trainsizes = trainsizes\n",
    "        self.eval_func = eval_func\n",
    "        self.opt_metric = opt_metric + '_mean'\n",
    "        self.maximize_opt_metric = maximize_opt_metric\n",
    "        self.rotation_skip = rotation_skip\n",
    "        \n",
    "        # Results attributes\n",
    "        # Full recording of all results for all paramsets, sizes, rotations,\n",
    "        # and metrics. This is a list of dictionaries for each paramset\n",
    "        self.results = None\n",
    "        # Validation summary report of all means and standard deviations for \n",
    "        # all metrics, for all paramsets, and sizes. This is a 3D s-by-r-by-p \n",
    "        # numpy array. Where s is the number of sizes, r the number of summary \n",
    "        # metrics +2, and p is the number of paramsets\n",
    "        self.report_by_size = None\n",
    "        # List of the indices of the best paramset for each size\n",
    "        self.best_param_inds = None\n",
    "\n",
    "    def perform_cross_validation(self, all_Xfolds, all_yfolds, \n",
    "                                 trainsize, verbose=0):\n",
    "        ''' TODO: FILL IN WITH YOUR SOLUTION FROM HW6\n",
    "        Perform cross validation for a singular train set size and single \n",
    "        hyper-parameter set, by evaluating the model's performance over \n",
    "        multiple data set rotations all of the same size.\n",
    "\n",
    "        NOTE: This function assumes the hyper-parameters have already been \n",
    "              set in the model\n",
    "            \n",
    "        PARAMS:\n",
    "            all_Xfolds: list containing all of the input data folds\n",
    "            all_yfolds: list containing all of the output data folds\n",
    "            trainsize: number of folds to use for training\n",
    "            verbose: flag to display simple debugging information\n",
    "            \n",
    "        RETURNS: train, val, and test set results for all rotations of the \n",
    "                 data sets and the summary (i.e. the averages over all the \n",
    "                 rotations) of the results. \n",
    "                 results is a dictionary of dictionaries of r-by-n numpy \n",
    "                 arrays. Where r is the number of rotations, and n is the \n",
    "                 number of outputs from the model.\n",
    "                 summary is a dictionary of dictionaries of 1-by-n numpy \n",
    "                 arrays. \n",
    "\n",
    "                 General form:\n",
    "                     results.keys() = ['train', 'val', 'test']\n",
    "\n",
    "                     results['train'].keys() = ['metric1', 'metric2', ...]\n",
    "                     \n",
    "                     results['train']['metric1'] = numpy_array\n",
    "                     \n",
    "                     results = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse'      : r_by_n_numpy_array,\n",
    "                                     'rmse_rads': r_by_n_numpy_array, \n",
    "                                     'rmse_degs': r_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "                     \n",
    "                     summary = \n",
    "                     {\n",
    "                        'train':\n",
    "                                 {\n",
    "                                     'mse_mean'      : 1_by_n_numpy_array,\n",
    "                                     'mse_std'       : 1_by_n_numpy_array,\n",
    "                                     'rmse_rads_mean': 1_by_n_numpy_array, \n",
    "                                     'rmse_rads_std' : 1_by_n_numpy_array,\n",
    "                                     ...\n",
    "                                 },\n",
    "                        'val'  : {...},\n",
    "                        'test' : {...}\n",
    "                     }\n",
    "\n",
    "                    For example, you can access the MSE results for the \n",
    "                    validation set like so:\n",
    "                        results['train'][metric] \n",
    "                    For example, you can access the summary (i.e. the average \n",
    "                    results over all the rotations) for the test set for the\n",
    "                    rMSE in degrees like so:\n",
    "                        summary['test']['rmse_degs_mean']                \n",
    "        '''\n",
    "        \n",
    "        # Verify a valid train set size was provided\n",
    "        nfolds = len(all_Xfolds)\n",
    "        if trainsize < 1 or trainsize > nfolds - 2: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.perform_cross_validation() - \"\n",
    "            err_msg += \"trainsize (%d) must be between 1 and nfolds (%d) - 2\" % (trainsize, nfolds)\n",
    "            raise ValueError(err_msg)\n",
    "            \n",
    "        # Verify rotation skip\n",
    "        if self.rotation_skip < 1: \n",
    "            err_msg = \"ERROR: KFoldHolisticCrossValidation.__init__() - \"\n",
    "            err_msg += \"rotation_skip (%d) can't be less than 1\" % self.rotation_skip\n",
    "            raise ValueError(err_msg)\n",
    "        \n",
    "        # Set up results recording for each rotation\n",
    "        results = {'train': None, 'val': None, 'test': None}\n",
    "        summary = {'train': {}, 'val': {}, 'test': {}}\n",
    "        \n",
    "        model = self.model\n",
    "        evaluate = self.eval_func\n",
    "        \n",
    "        # Rotate through different train, val, and test sets\n",
    "        for rotation in range(0, nfolds, self.rotation_skip):\n",
    "            # Determine fold indices for train, val, and test set. \n",
    "            # The val and tests are each only 1 fold\n",
    "            \n",
    "            \n",
    "        \n",
    "            # Construct train set by concatenating the individual  \n",
    "            # training folds\n",
    "            \n",
    "            \n",
    "\n",
    "            # Construct validation set\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Construct test set\n",
    "            \n",
    "            \n",
    "            \n",
    "            # DEBUGGING\n",
    "            if verbose:\n",
    "                print(\"TRAIN\", X.shape, y.shape, trainfolds)\n",
    "                print(\"VAL\", Xval.shape, yval.shape, valfold)\n",
    "                print(\"TEST\", Xtest.shape, ytest.shape, testfold)\n",
    "            \n",
    "            # Train model using the training set\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Predict with the model for train, val, and test sets\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Evaluate the model for each set\n",
    "            \n",
    "            \n",
    "\n",
    "            # Record the train, val, and test set results. These are dicts \n",
    "            # of result metrics, returned by the evaluate function\n",
    "            # For the first rotation, store the results from evaluating\n",
    "            # with the train, val, and tests by setting the values of   \n",
    "            # the appropriate items within the results dict\n",
    "            if results['train'] is None: \n",
    "                \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                # Append the results for each rotation\n",
    "                for metric in res_train.keys():\n",
    "                    results['train'][metric] = np.append(results['train'][metric], \n",
    "                                                         res_train[metric], axis=0)\n",
    "                    results['val'][metric] = np.append(results['val'][metric], \n",
    "                                                       res_val[metric], axis=0)\n",
    "                    results['test'][metric] = np.append(results['test'][metric], \n",
    "                                                        res_test[metric], axis=0)\n",
    "\n",
    "        # Compute/record mean and standard deviation for the size for each metric\n",
    "        for metric in results['train'].keys():\n",
    "            for stat_set in ['train', 'val', 'test']:\n",
    "                summary[stat_set][metric+'_mean'] = np.mean(results[stat_set][metric], \n",
    "                                                            axis=0).reshape(1, -1)\n",
    "                summary[stat_set][metric+'_std'] = np.std(results[stat_set][metric], \n",
    "                                                          axis=0).reshape(1, -1)\n",
    "\n",
    "        return results, summary\n",
    "\n",
    "    def grid_cross_validation(self, all_Xfolds, all_yfolds, verbose=0):\n",
    "        '''\n",
    "        (MAIN PROCEDURE) Perform cross validation for multiple sets of \n",
    "        parameters and train set sizes. Calls self.perform_cross_validation(). \n",
    "        This is the procedure that executes cross validation for all parameter \n",
    "        sets and all sizes.\n",
    "        \n",
    "        General Procedure:\n",
    "        + iter over hyper-parameter sets\n",
    "          1. set hyper-parameters of the model\n",
    "          2. iter over train set sizes\n",
    "             a. iter over splits/rotations\n",
    "                  i. train the model\n",
    "                 ii. evaluate the model on train, val, and test sets\n",
    "                iii. record the results\n",
    "             b. record the results by size\n",
    "          3. record the results by hyper-parameter set\n",
    "        \n",
    "        PARAMS:\n",
    "            all_Xfolds: all the input data folds (list of folds, as it was \n",
    "                        loaded from the files)\n",
    "            all_yfolds: all the output data folds (list of folds)\n",
    "            verbose: flag to print out simple debugging information\n",
    "            \n",
    "        RETURNS: best parameter set for each train set size as a list of \n",
    "                 parameter indices. Additionally, returns self.report_by_size,\n",
    "                 the 3D array of validation means (overall rotations) for all \n",
    "                 paramsets, for each metric, for all sizes. The structure of \n",
    "                 the returned object is a dictionary of the following form: \n",
    "                 { \n",
    "                   'report_by_size' : self.report_by_size, \n",
    "                   'best_param_inds': self.best_param_inds\n",
    "                 }\n",
    "        ''' \n",
    "        sizes = self.trainsizes\n",
    "        paramsets = self.paramsets\n",
    "        nparamsets = len(paramsets)\n",
    "        print(\"nparamsets\", nparamsets)\n",
    "        \n",
    "        # Set up all results\n",
    "        all_results = []\n",
    "        \n",
    "        # Iterate over parameter sets\n",
    "        for params in paramsets:\n",
    "            # Set up paramset results \n",
    "            param_res = []\n",
    "            param_smry = None\n",
    "            \n",
    "            # Set model parameters\n",
    "            print(\"Current paramset\\n\", params)\n",
    "            self.model.set_params(**params)\n",
    "\n",
    "            # Iterate over the different train set sizes\n",
    "            for size in sizes:\n",
    "                # Cross-validation for current model and train size\n",
    "                res, smry = self.perform_cross_validation(all_Xfolds, \n",
    "                                                          all_yfolds, \n",
    "                                                          size, verbose)\n",
    "\n",
    "                # Save the results\n",
    "                param_res.append(res) \n",
    "                # Save the mean and standard deviation statistics (summary)\n",
    "                if param_smry is None: param_smry = smry\n",
    "                else:\n",
    "                    # For each metric measured, append the summary results\n",
    "                    for metric in smry['train'].keys():\n",
    "                        for stat_set in ['train', 'val', 'test']:\n",
    "                            stat = smry[stat_set][metric]\n",
    "                            param_smry[stat_set][metric] = np.append(param_smry[stat_set][metric],\n",
    "                                                                     stat, axis=0)\n",
    "            \n",
    "            # Append the results and summary for the parameter set\n",
    "            all_results.append({'params':params, 'results':param_res, \n",
    "                                'summary':param_smry})\n",
    "        \n",
    "        # Generate reports and determine best params for each size \n",
    "        self.results = all_results\n",
    "        self.report_by_size = self.get_reports()\n",
    "        self.best_param_inds = self.get_best_params(self.opt_metric, \n",
    "                                                    self.maximize_opt_metric)\n",
    "        return {'report_by_size':self.report_by_size, \n",
    "                'best_param_inds':self.best_param_inds}\n",
    "\n",
    "    def get_reports(self):\n",
    "        ''' \n",
    "        Get the mean validation summary of all the parameters for each size\n",
    "        for all metrics. This is used to determine the best parameter set  \n",
    "        for each size\n",
    "        \n",
    "        RETURNS: the report_by_size as a 3D s-by-r-by-p array. Where s is \n",
    "                 the number of train sizes tried, r is the number of summary  \n",
    "                 metrics evaluated+2, and p is the number of parameter sets.\n",
    "        '''\n",
    "        results = self.results\n",
    "        sizes = np.reshape(self.trainsizes, (1, -1))\n",
    "        \n",
    "        nsizes = sizes.shape[1]\n",
    "        nparams = len(results)\n",
    "        \n",
    "        # Set up the reports objects\n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics \n",
    "        report_by_size = np.empty((nsizes, len(colnames), nparams), dtype=object)\n",
    "\n",
    "        # Determine mean val for each paramset for each size for all metrics\n",
    "        for p, paramset_result in enumerate(results):\n",
    "            params = paramset_result['params']\n",
    "            res_val = paramset_result['summary']['val']\n",
    "\n",
    "            # Compute mean val result for each train size for each metric\n",
    "            means_by_size = [np.mean(res_val[metric], axis=1) \n",
    "                             for metric in metrics]\n",
    "            # Include the train set sizes into the report\n",
    "            means_by_size = np.append(sizes, means_by_size, axis=0)\n",
    "            # Include the parameter sets into the report\n",
    "            param_strgs = np.reshape([str(params)]*nsizes, (1, -1))\n",
    "            means_by_size = np.append(param_strgs, means_by_size, axis=0).T\n",
    "            # Append the parameter set means into the report \n",
    "            report_by_size[:,:,p] = means_by_size\n",
    "        return report_by_size\n",
    "\n",
    "    def get_best_params(self, opt_metric, maximize_opt_metric):\n",
    "        ''' \n",
    "        Determines the best parameter set for each train size,  \n",
    "        based on a specific metric.\n",
    "        \n",
    "        PARAMS:\n",
    "            opt_metric: optimized metric. one of the metrics returned \n",
    "                        from eval_func, with '_mean' appended for the\n",
    "                        summary stat. This is the mean metric used to  \n",
    "                        determine the best parameter set for each size\n",
    "                        \n",
    "            maximize_opt_metric: True if the max of opt_metric should be\n",
    "                                 used to determine the best parameters.\n",
    "                                 False if the min should be used.\n",
    "        RETURNS: list of best parameter set indicies for each size \n",
    "        '''\n",
    "        results = self.results\n",
    "        report_by_size = self.report_by_size \n",
    "                \n",
    "        metrics = list(results[0]['summary']['val'].keys())\n",
    "        \n",
    "        # Determine best params for each size, for the optimized metric\n",
    "        best_param_inds = None\n",
    "        metric_idx = metrics.index(opt_metric)\n",
    "        \n",
    "        # Report info for all paramsets for the optimized metric\n",
    "        report_opt_metric = report_by_size[:, metric_idx+2, :]\n",
    "        \n",
    "        if maximize_opt_metric:\n",
    "            # Add two for the additional cols for params and size\n",
    "            best_param_inds = np.argmax(report_opt_metric, axis=1)\n",
    "        else: \n",
    "            best_param_inds = np.argmin(report_opt_metric, axis=1)\n",
    "        # Return list of best params indices for each size\n",
    "        return best_param_inds\n",
    "    \n",
    "    def get_best_params_strings(self):\n",
    "        ''' \n",
    "        Generates a list of strings of the best params for each size\n",
    "        RETURNS: list of strings of the best params for each size\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        results = self.results\n",
    "        return [str(results[p]['params']) for p in best_param_inds]\n",
    "\n",
    "    def get_report_best_params_for_size(self, size):\n",
    "        ''' \n",
    "        Get the mean validation summary for the best parameter set \n",
    "        for a specific size for all metrics.\n",
    "        PARAMS:\n",
    "            size: index of desired train set size for the best  \n",
    "                  paramset to come from. Size here is the index in \n",
    "                  the trainsizes list, NOT the actual number of folds.\n",
    "        RETURNS: the best parameter report for the size as an s-by-m  \n",
    "                 dataframe. Where each row is for a different size, and \n",
    "                 each column is for a different summary metric.\n",
    "        '''\n",
    "        best_param_inds = self.best_param_inds\n",
    "        report_by_size = self.report_by_size \n",
    "\n",
    "        # Obtain the index of the best parameter set for the size\n",
    "        bp_index = best_param_inds[size]\n",
    "\n",
    "        # Obtain the list of metrics\n",
    "        metrics = list(self.results[0]['summary']['val'].keys())\n",
    "        colnames = ['params', 'size'] + metrics\n",
    "        \n",
    "        # Create DataFame with all summary stats for the parameter set\n",
    "        report_best_params_for_size = pd.DataFrame(report_by_size[:,:,bp_index],\n",
    "                                                   columns=colnames)\n",
    "        return report_best_params_for_size\n",
    "\n",
    "    def plot_cv(self, foldsindices, results, summary, metrics, size):\n",
    "        ''' \n",
    "        Plotting function for after perform_cross_validation(), \n",
    "        displaying the train and val set performances for each rotation \n",
    "        of the training set. \n",
    "        \n",
    "        PARAMS:\n",
    "            foldsindices: indices of the train sets tried\n",
    "            results: results from perform_cross_validation()\n",
    "            summary: mean and standard deviations of the results\n",
    "            metrics: list of result metrics to plot. Available metrics \n",
    "                     are the keys in the dict returned by eval_func\n",
    "            size: train set size\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.35)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(results['train'][metric], axis=1)\n",
    "            res_val = np.mean(results['val'][metric], axis=1)\n",
    "            #res_test = np.mean(results['test'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(foldsindices, res_train, label='train')\n",
    "            ax.plot(foldsindices, res_val, label='val')\n",
    "            #ax.plot(foldsindices, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[0].legend(loc='upper right')\n",
    "        axs[0].set(xlabel='Fold Index')\n",
    "        axs[0].set(title='Performance for Train Set Size ' + str(size))\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_param_train_val(self, metrics, paramidx=0, view_test=False):\n",
    "        ''' \n",
    "        Plotting function for after grid_cross_validation(), \n",
    "        displaying the mean (summary) train and val set performances \n",
    "        for each train set size.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std'\n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "            paramidx: parameter set index\n",
    "            view_test: flag to view the test set results\n",
    "            \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "\n",
    "        summary = results[paramidx]['summary']\n",
    "        params = results[paramidx]['params']\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(12,6))\n",
    "        fig.subplots_adjust(hspace=.35)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            # Compute the mean for multiple outputs\n",
    "            res_train = np.mean(summary['train'][metric], axis=1)\n",
    "            res_val = np.mean(summary['val'][metric], axis=1)\n",
    "            # Plot\n",
    "            ax.plot(sizes, res_train, label='train')\n",
    "            ax.plot(sizes, res_val, label='val')\n",
    "            if view_test:\n",
    "                res_test = np.mean(summary['test'][metric], axis=1)\n",
    "                ax.plot(sizes, res_test, label='test')\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title=str(params))\n",
    "        axs[0].legend(loc='upper right')\n",
    "        return fig, axs\n",
    "    \n",
    "    def plot_allparams_val(self, metrics):\n",
    "        ''' \n",
    "        Plotting function for after grid_cross_validation(), displaying  \n",
    "        mean (summary) validation set performances for each train size \n",
    "        for all parameter sets for the specified metrics.\n",
    "        \n",
    "        PARAMS:\n",
    "            metrics: list of summary metrics to plot. '_mean' or '_std' \n",
    "                     must be append to the end of the base metric name. \n",
    "                     These base metric names are the keys in the dict \n",
    "                     returned by eval_func\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        sizes = self.trainsizes\n",
    "        results = self.results\n",
    "        \n",
    "        nmetrics = len(metrics)\n",
    "\n",
    "        # Initialize figure plots\n",
    "        fig, axs = plt.subplots(nmetrics, 1, figsize=(10,6))\n",
    "        fig.subplots_adjust(hspace=.35)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "\n",
    "        # Construct each subplot\n",
    "        for metric, ax in zip(metrics, axs):\n",
    "            for p, param_results in enumerate(results):\n",
    "                summary = param_results['summary']\n",
    "                params = param_results['params']\n",
    "                # Compute the mean for multiple outputs\n",
    "                res_val = np.mean(summary['val'][metric], axis=1)                \n",
    "                ax.plot(sizes, res_val, label=str(params))\n",
    "            ax.set(ylabel=metric)\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].set(title='Validation Performance')\n",
    "        axs[0].legend(bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 8})\n",
    "        return fig, axs\n",
    "\n",
    "    def plot_best_params_by_size(self):\n",
    "        ''' \n",
    "        Plotting function for after grid_cross_validation(), displaying \n",
    "        mean (summary) train and validation set performances for the best \n",
    "        parameter set for each train size for the optimized metric.\n",
    "                     \n",
    "        RETURNS: the figure and axes handles\n",
    "        '''\n",
    "        results = self.results\n",
    "        metric = self.opt_metric\n",
    "        best_param_inds = self.best_param_inds\n",
    "        sizes = np.array(self.trainsizes)\n",
    "\n",
    "        # Unique set of best params for the legend\n",
    "        unique_param_sets = np.unique(best_param_inds)\n",
    "        lgnd_params = [self.paramsets[p] for p in unique_param_sets]\n",
    "\n",
    "        # Initialize figure\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(10,6))\n",
    "        fig.subplots_adjust(hspace=.35)\n",
    "        # When 1 metric is provided, allow the axs to be iterable\n",
    "        axs = np.array(axs).ravel()\n",
    "        set_names = ['train', 'val']\n",
    "\n",
    "        # Construct each subplot\n",
    "        for i, (ax, set_name) in enumerate(zip(axs, set_names)):\n",
    "            for p in unique_param_sets:\n",
    "                # Obtain indices of sizes this paramset was best for\n",
    "                param_size_inds = np.where(best_param_inds == p)[0]\n",
    "                param_sizes = sizes[param_size_inds]\n",
    "                # Compute the mean over multiple outputs for each size\n",
    "                param_summary = results[p]['summary'][set_name]\n",
    "                metric_scores = np.mean(param_summary[metric][param_size_inds, :], axis=1)\n",
    "                # Plot the param results for each size it was the best for\n",
    "                ax.scatter(param_sizes, metric_scores, s=120, marker=(p+2, 1))\n",
    "                #ax.grid(True)\n",
    "\n",
    "            set_name += ' Set Performance'\n",
    "            ax.set(ylabel=metric, title=set_name)\n",
    "\n",
    "        axs[-1].set(xlabel='Train Set Size (# of folds)')\n",
    "        axs[0].legend(lgnd_params, bbox_to_anchor=(1.02, 1), loc='upper left',\n",
    "                      ncol=1, borderaxespad=0., prop={'size': 7})\n",
    "        return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORM CROSS VALIDATION\n",
    "Initialize holistic cross validation objects to explore Linear, Ridge, Lasso, and ElasticNet models.\n",
    "\n",
    "The experiments for the ElasticNet have been provided in a file (hw7_full_crossval.pkl) due to the length of time it takes to run; however, you are welcome to re-run these experiements, for all/various train set sizes, and rotations, using score_eval as the eval_func, and rmse_degs as the metric to optimize. The file can be found in the hw7 folder in the ml_practices directory, along with this notebook.\n",
    "\n",
    "The inputs for the models are the MI data and the outputs are the torque (you'll provide the shoulder and elbow simulataneouly, as done in the previous HW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Holistic Cross Validation Options:\n",
    "* ridge_alphas: list of alphas to try for the RIDGE model\n",
    "* lasso_alphas: list of alphas to try for the LASSO model\n",
    "* en_alphas: list of alphas to try for the ELASTICNET model\n",
    "* l1_ratios: list of l1_ratios to try for the ELASTICNET model\n",
    "\n",
    "* trainsizes: list of number of folds to utilize in the train set\n",
    "* opt_metric: the optimized metric, returned by the eval_func, used \n",
    "  to select the best parameter sets\n",
    "* maximize_opt_metric: True if the opt_metric is maximized; False \n",
    "  otherwise\n",
    "* skip: the number of folds to skip when rotating through train sets \n",
    "  of the same size\n",
    "\"\"\"\n",
    "ridge_alphas = [1, 10, 50, 100, 500, 1000, 10000]\n",
    "lasso_alphas = [.001, .005, .01, .025, .05, .075, .1]\n",
    "en_alphas = lasso_alphas + [0.5, 1]\n",
    "l1_ratios = [0.001, .025, .05, .1, .5, 1]\n",
    "\n",
    "trainsizes = range(1, nfolds-1)\n",
    "opt_metric = 'rmse_degs'\n",
    "maximize_opt_metric = False\n",
    "skip = 1\n",
    "\n",
    "# True to always run cross validation, false to re-load existing run\n",
    "# or run cross validation for the first time\n",
    "force = False \n",
    "# Tag for the filename to save the experiments to\n",
    "prefix = \"_full\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LINEAR REGRESSION](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "Ordinary least squares Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LinearRegression\n",
    "\n",
    "Execute cross validation procedure for all sizes for the \n",
    "LinearRegression model using grid_cross_validation().\n",
    "The parameter list for the LinearRegression model is a\n",
    "list with just an empty dictionary [{}]\n",
    "\"\"\"\n",
    "lnr_fullcvfname = \"hw7\" + prefix + \"_linear_crossval.pkl\"\n",
    "\n",
    "model = LinearRegression()\n",
    "lnr_crossval = KFoldHolisticCrossValidation(model, [{}], score_eval, \n",
    "                                            opt_metric, maximize_opt_metric,\n",
    "                                            trainsizes, skip)\n",
    "\n",
    "lnr_crossval_report = None\n",
    "if force or (not os.path.exists(lnr_fullcvfname)):\n",
    "    # TODO: Execute cross validation procedure for all parameters and sizes\n",
    "    lnr_crossval_report = # TODO\n",
    "    # TODO: Save the cross validation object, use joblib.dump()\n",
    "    \n",
    "    \n",
    "else:\n",
    "    # Re-load saved crossval object instead of re-running \n",
    "    lnr_crossval = joblib.load(lnr_fullcvfname)\n",
    "    lnr_crossval_report = {'report_by_size': lnr_crossval.report_by_size,\n",
    "                         'best_param_inds': lnr_crossval.best_param_inds}\n",
    "\n",
    "lnr_crossval.model, lnr_crossval.rotation_skip, lnr_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [RIDGE](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression)\n",
    "\n",
    "$\\min_w ||y - w^TX||^2_2 + \\alpha ||w||^2_2$\n",
    "\n",
    "$\\alpha$: amount of $L_2$ regularization to apply. Larger $\\alpha$ greater penalize the model for larger weights\n",
    "\n",
    "$w$: the weights from the model\n",
    "\n",
    "$X$: feature or input data\n",
    "\n",
    "$y$: true outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "\n",
    "Initialize a KFoldHolisticCrossValidation object that uses RIDGE\n",
    "as the model, and the provided r_allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the Ridge\n",
    "model using grid_cross_validation()\n",
    "\"\"\"\n",
    "r_fullcvfname = \"hw7\" + prefix + \"_ridge_crossval.pkl\"\n",
    "\n",
    "r_param_lists = {'alpha':ridge_alphas, 'max_iter':[1e4]}\n",
    "r_allparamsets = generate_paramsets(r_param_lists)\n",
    "print(pd.DataFrame(r_allparamsets))\n",
    "\n",
    "model = Ridge()\n",
    "# TODO: Initialize a KFoldHolisticCrossValidation object using Ridge\n",
    "r_crossval = # TODO\n",
    "\n",
    "r_crossval_report = None\n",
    "if force or (not os.path.exists(r_fullcvfname)):\n",
    "    # TODO: Execute cross validation for all parameters and sizes\n",
    "    r_crossval_report = # TODO\n",
    "    # TODO: Save the cross validation object\n",
    "    \n",
    "\n",
    "else:\n",
    "    # Re-load saved crossval object instead of re-running \n",
    "    r_crossval = joblib.load(r_fullcvfname)\n",
    "    r_crossval_report = {'report_by_size' : r_crossval.report_by_size,\n",
    "                         'best_param_inds': r_crossval.best_param_inds}\n",
    "\n",
    "r_crossval.model, r_crossval.rotation_skip, r_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LASSO](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n",
    "$\\min_w \\frac{1}{2 N} ||y - w^TX||^2_2 + \\alpha ||w||_1$\n",
    "\n",
    "$N$: the number of samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "\n",
    "Initialize a KFoldHolisticCrossValidation object that uses LASSO\n",
    "as the model, and the provided l_allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the Lasso\n",
    "model using grid_cross_validation()\n",
    "\"\"\"\n",
    "l_fullcvfname = \"hw7\" + prefix + \"_lasso_crossval.pkl\"\n",
    "\n",
    "l_param_lists = {'alpha':lasso_alphas, 'max_iter':[1e4]}\n",
    "l_allparamsets = generate_paramsets(l_param_lists)\n",
    "print(pd.DataFrame(l_allparamsets))\n",
    "\n",
    "model = Lasso()\n",
    "# TODO: Initialize a KFoldHolisticCrossValidation object using Lasso\n",
    "l_crossval = # TODO\n",
    "\n",
    "l_crossval_report = None\n",
    "if force or (not os.path.exists(l_fullcvfname)):\n",
    "    # TODO: Execute cross validation for all parameters and sizes\n",
    "    l_crossval_report = # TODO\n",
    "    # TODO: Save the cross validation object\n",
    "    \n",
    "    \n",
    "else:\n",
    "    # Re-load saved crossval object instead of re-running \n",
    "    l_crossval = joblib.load(l_fullcvfname)\n",
    "    l_crossval_report = {'report_by_size' : l_crossval.report_by_size,\n",
    "                         'best_param_inds': l_crossval.best_param_inds}\n",
    "\n",
    "l_crossval.model, l_crossval.rotation_skip, l_crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ELASTICNET](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)\n",
    "\n",
    "$\\min_w \\frac{1}{2 N} ||y - w^TX||^2_2 + \\alpha L_1 ||w||_1 + \\frac{1}{2} \\alpha (1 - L_1) ||w||^2_2$\n",
    "\n",
    "$L_1$: the $L_1$ ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "\n",
    "Initialize a KFoldHolisticCrossValidation object that uses ELASTICNET\n",
    "as the model, and the provided allparamsets\n",
    "\n",
    "Execute cross validation procedure for all sizes for the ELASTICNET\n",
    "model using grid_cross_validation()\n",
    "\n",
    "Re-load the existing experiment\n",
    "\"\"\"\n",
    "fullcvfname = \"hw7\" + prefix + \"_crossval.pkl\"\n",
    "\n",
    "param_lists = {'alpha':en_alphas, 'l1_ratio':l1_ratios, 'max_iter':[1e4]}\n",
    "allparamsets = generate_paramsets(param_lists)\n",
    "nparamsets = len(allparamsets)\n",
    "print(pd.DataFrame(allparamsets))\n",
    "\n",
    "model = ElasticNet()\n",
    "crossval = KFoldHolisticCrossValidation(model, allparamsets, score_eval, \n",
    "                                        opt_metric, maximize_opt_metric,\n",
    "                                        trainsizes, skip)\n",
    "\n",
    "crossval_report = None\n",
    "if force or (not os.path.exists(fullcvfname)):\n",
    "    # Execute cross validation for all parameters and sizes\n",
    "    crossval_report = crossval.grid_cross_validation(MI_folds, \n",
    "                                                     torque_folds, \n",
    "                                                     verbose=0)\n",
    "    # Save the cross validation object\n",
    "    joblib.dump(crossval, fullcvfname)\n",
    "else:\n",
    "    # TODO: Re-load saved crossval object. Use joblib.load()\n",
    "    crossval = # TODO\n",
    "    crossval_report = {'report_by_size' : crossval.report_by_size, \n",
    "                       'best_param_inds': crossval.best_param_inds}\n",
    "\n",
    "crossval.model, crossval.rotation_skip, crossval.trainsizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the result output structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List KFoldHolisticCrossValidation Attributes\n",
    "\"\"\"\n",
    "dir(crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Results attribute is a list of dictionaries. Each element, or dictionary\n",
    "corresponds to the results for a single parameter set\n",
    "\"\"\"\n",
    "len(crossval.results), crossval.results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'] is a list of dictionaries with the results\n",
    "  for each size for the parameter set at index 0\n",
    "* crossval.results[1]['summary'] is a dictionary of summary results for the \n",
    "  train, val, and test sets for the parameter set at index 1\n",
    "\"\"\"\n",
    "len(crossval.results[0]['results']), crossval.results[1]['summary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2] is a dictionary with the results\n",
    "  for the train size at index 2 for the parameter set at index 0\n",
    "* crossval.results[1]['summary']['val'] is a dictionary of summary (over the \n",
    "  sizes) results for the val set for the parameter set at index 1, for all \n",
    "  metrics\n",
    "\"\"\"\n",
    "crossval.results[0]['results'][2].keys(), crossval.results[1]['summary']['val'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2]['train'] is a dictionary of all results for \n",
    "  the train set for the parameter set at index 0, the size at index 2, for all\n",
    "  metrics\n",
    "* crossval.results[1]['summary']['val']['mse_mean'] is a numpy array of averages \n",
    "  for the val set for the parameter set at index 1, for the mse. The averages are \n",
    "  computed over the sizes\n",
    "\"\"\"\n",
    "crossval.results[0]['results'][2]['train'].keys(), crossval.results[1]['summary']['val']['mse_mean'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "* crossval.results[0]['results'][2]['train']['mse'] is a dictionary of all \n",
    "  results for the train set for the parameter set at index 0, the size at \n",
    "  index 2, for the mse, for all rotations (there are 20 rotations when skip=1)\n",
    "\"\"\"\n",
    "crossval.results[0]['results'][2]['train']['mse'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters for Each Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Results options:\n",
    "* size_idx: index of the size from the list of train sizes to examine results\n",
    "* metrics: list of summary (average) metrics to examine results\n",
    "\"\"\"\n",
    "# index 7 corresponds to train size 8\n",
    "size_idx = 7 \n",
    "metrics = ['rmse_degs_mean', 'evar_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Display the lists of the best parameter sets for each size for all\n",
    "the models, expect the Linear model (as it has only one parameter set)\n",
    "\"\"\"\n",
    "print(\"Best Parameter Sets For Each Train Set Size\")\n",
    "\n",
    "print(\"RIDGE\")\n",
    "r_best_param_info = pd.DataFrame((r_crossval.trainsizes, \n",
    "                                  r_crossval.best_param_inds, \n",
    "                                  r_crossval.get_best_params_strings()),\n",
    "                                  index=['train_size','param_index','paramset'])\n",
    "print(r_best_param_info.T)\n",
    "\n",
    "\n",
    "print(\"LASSO\")\n",
    "l_best_param_info = pd.DataFrame((l_crossval.trainsizes, \n",
    "                                  l_crossval.best_param_inds, \n",
    "                                  l_crossval.get_best_params_strings()),\n",
    "                                  index=['train_size','param_index','paramset'])\n",
    "print(l_best_param_info.T)\n",
    "\n",
    "print(\"ELASTICNET\")\n",
    "best_param_info = pd.DataFrame((crossval.trainsizes, \n",
    "                                crossval.best_param_inds, \n",
    "                                crossval.get_best_params_strings()),\n",
    "                                index=['train_size', 'param_index', 'paramset'])\n",
    "print(best_param_info.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Best Parameters for Each Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "LINEAR REGRESSION\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "each train size for the optimized metric. Use plot_best_params_by_size()\n",
    "\n",
    "Note: for LinearRegression, there is only one parameter set.\n",
    "\"\"\"\n",
    "lnr_crossval.plot_best_params_by_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "Plot the mean (summary) train and validation set performances for \n",
    "the best parameter set for each train size for the optimized\n",
    "metrics. Use plot_best_params_by_size()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Validation for All Parameter Sets for Each Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LINEAR REGRESSION\n",
    "Plot the validation results for all parameter sets over all train \n",
    "sizes, for the specified metrics, rmse_degs_mean and evar_mean\n",
    "(this variable is declared above). Use plot_allparams_val()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "Plot the validation results for all parameter sets over all train \n",
    "sizes, for the specified metrics, rmse_degs_mean and evar_mean\n",
    "(this variable is declared above). Use plot_allparams_val()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "Plot the validation results for all parameter sets over all train \n",
    "sizes, for the specified metrics, rmse_degs_mean and evar_mean\n",
    "(this variable is declared above). Use plot_allparams_val()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "Plot the validation results for all parameter sets over all train \n",
    "sizes, for the specified metrics, rmse_degs_mean and evar_mean\n",
    "(this variable is declared above). Use plot_allparams_val()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the TRAIN and VAL Set Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LINEAR REGRESSION\n",
    "For the best parameter set for the train set size at \n",
    "size_idx=7 (this variable has already been declared above),\n",
    "plot the TRAIN and VAL set performances using \n",
    "plot_param_train_val() for just the optimized metric.\n",
    "\n",
    "Note: there is only one parameter set for the Linear model, \n",
    "thus paramidx=0\n",
    "\"\"\"\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "For the best parameter set for the train set size at \n",
    "size_idx=7 (this variable has already been declared above),\n",
    "plot the TRAIN and VAL set performances using \n",
    "plot_param_train_val() for just the optimized metric\n",
    "\n",
    "Use r_crossval.best_param_inds to get the desired parameter \n",
    "set index\n",
    "\"\"\"\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "For the best parameter set for the train set size at \n",
    "size_idx=7 (this variable has already been declared above),\n",
    "plot the TRAIN and VAL set performances using \n",
    "plot_param_train_val() for just the optimized metric\n",
    "\"\"\"\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ELASTICNET\n",
    "For the best parameter set for the train set size at \n",
    "size_idx=7 (this variable has already been declared above),\n",
    "plot the TRAIN and VAL set performances using \n",
    "plot_param_train_val() for just the optimized metric\n",
    "\"\"\"\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "bp_idx = crossval.best_param_inds[size_idx]\n",
    "crossval.plot_param_train_val([crossval.opt_metric], paramidx=bp_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance over the Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_val_for_size(crossval, metric, alphas, sizeidx=0):\n",
    "    ''' PROVIDED\n",
    "    Plotting function for after grid_cross_validation(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha, given the size, for RIDGE and LASSO only\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        alphas: list of alpha values\n",
    "        sizeidx: train size index\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "\n",
    "    nalphas = len(alphas)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "    nmetrics = len(metrics)\n",
    "\n",
    "    # Initialize the matrices for the curve\n",
    "    Y_train = np.empty((nalphas,))\n",
    "    Y_val = np.empty((nalphas,))\n",
    "\n",
    "    # Obtain the mean performance for the curve\n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric][sizeidx, :])\n",
    "        Y_train[alpha_idx] = res_train\n",
    "\n",
    "        res_val = np.mean(summary['val'][metric][sizeidx, :])\n",
    "        Y_val[alpha_idx] = res_val\n",
    "    \n",
    "    # Initialize figure plots\n",
    "    fig = plt.figure(figsize=(12,2))\n",
    "    for i, (Y, set_name) in enumerate(zip((Y_train, Y_val), \n",
    "                                          ('Training', 'Validation'))):\n",
    "        # Plot\n",
    "        ax = fig.add_subplot(1, 2, i+1)\n",
    "        ax.plot(alphas, Y)\n",
    "        title = \"%s Performance, Train Size %d Folds\" % (set_name, sizes[sizeidx])\n",
    "        ax.set(title=title)\n",
    "        ax.set(xlabel=r\"$\\alpha$\", ylabel=metric)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_surface(xlist, ylist, Z_train, Z_val, ylabel, zlabel, \n",
    "                 elev=30, angle=45, title_suffix=\"\"):\n",
    "    ''' PROVIDED\n",
    "    Helper plotting function. x-axis is always alpha\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        xlist: list of x values\n",
    "        ylist: list of y values\n",
    "        Z_train: matrix of performance results from the training set\n",
    "        Z_val: matrix of performance results from the validation set\n",
    "        ylabel: y-axis label \n",
    "        zlabel: z-axis label\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "        title_suffix: string to append to each subplot title\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    # Initialize figure\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    X, Y = np.meshgrid(xlist, ylist) \n",
    "    for i, (Z, set_name) in enumerate(zip((Z_train, Z_val), \n",
    "                                          ('Training', 'Validation'))):\n",
    "        # Plot the surface\n",
    "        ax = fig.add_subplot(1, 2, i+1, projection='3d')\n",
    "        surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, \n",
    "                               linewidth=0, antialiased=False)\n",
    "        title = \"%s Performance %s\" % (set_name, title_suffix)\n",
    "        ax.view_init(elev=elev, azim=angle)\n",
    "        ax.set(title=title)\n",
    "        ax.set(xlabel=r\"$\\alpha$\", ylabel=ylabel, zlabel=zlabel)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_val_surface_RL(crossval, metric, alphas, elev=30, angle=245):\n",
    "    ''' PROVIDED\n",
    "    Plotting function for after grid_cross_validation(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha, for all sizes, for RIDGE and LASSO only\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        alphas: list of alpha values\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "\n",
    "    nalphas = len(alphas)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "    nmetrics = len(metrics)\n",
    "\n",
    "    # Initialize the matrices for the surface\n",
    "    Z_train = np.empty((nsizes, nalphas))\n",
    "    Z_val = np.empty((nsizes, nalphas))\n",
    "\n",
    "    # Obtain the mean performance for the surface\n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric], axis=1)\n",
    "        Z_train[:, alpha_idx] = res_train\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_val = np.mean(summary['val'][metric], axis=1)\n",
    "        Z_val[:, alpha_idx] = res_val\n",
    "    \n",
    "    fig = plot_surface(alphas, sizes, Z_train, Z_val, 'size (# of folds)', \n",
    "                       metric, elev, angle)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_param_val_surface_EN(crossval, metric, param_lists, \n",
    "                              sizeidx=0, elev=35, angle=280):\n",
    "    ''' PROVIDED\n",
    "    Plotting function for after grid_cross_validation(), \n",
    "    displaying the mean (summary) train and val set performances \n",
    "    for each alpha and l1_ratio, given the size, for the ELASTICNET\n",
    "    \n",
    "    REQUIRES: from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    PARAMS:\n",
    "        crossval: cross validation object\n",
    "        metric: summary metric to plot. '_mean' or '_std' must be \n",
    "                append to the end of the base metric name. These \n",
    "                base metric names are the keys in the dict returned\n",
    "                by eval_func\n",
    "        param_lists: dictionary of the list of alphas and l1_ratios\n",
    "        sizeidx: train size index\n",
    "        elev: elevation of the 3D plot for the view\n",
    "        angle: angle in degrees of the 3D plot for the view\n",
    "\n",
    "    RETURNS: the figure and axes handles\n",
    "    '''\n",
    "    sizes = crossval.trainsizes\n",
    "    results = crossval.results\n",
    "    best_param_inds = crossval.best_param_inds\n",
    "\n",
    "    alphas = list(param_lists['alpha'])\n",
    "    l1_ratios = list(param_lists['l1_ratio'])\n",
    "\n",
    "    nalphas = len(alphas)\n",
    "    nl1_ratios = len(l1_ratios)\n",
    "\n",
    "    nsizes = len(sizes)\n",
    "    nmetrics = len(metrics)\n",
    "\n",
    "    # Initialize the matrices for the surface\n",
    "    Z_train = np.empty((nl1_ratios, nalphas))\n",
    "    Z_val = np.empty((nl1_ratios, nalphas))\n",
    "\n",
    "    # Obtain the mean performance for the surface \n",
    "    for param_res in results:\n",
    "        params = param_res['params']\n",
    "        summary = param_res['summary']\n",
    "\n",
    "        alpha_idx = alphas.index(params['alpha'])\n",
    "        l1_idx = l1_ratios.index(params['l1_ratio'])\n",
    "\n",
    "        # Compute the mean for multiple outputs\n",
    "        res_train = np.mean(summary['train'][metric][sizeidx, :])\n",
    "        Z_train[l1_idx, alpha_idx] = res_train\n",
    "\n",
    "        res_val = np.mean(summary['val'][metric][sizeidx, :])\n",
    "        Z_val[l1_idx, alpha_idx] = res_val\n",
    "    \n",
    "    fig = plot_surface(alphas, l1_ratios, Z_train, Z_val, 'l1_ratio', \n",
    "                       metric, elev, angle,', Size %d Folds' % sizes[sizeidx])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for RIDGE\n",
    "\"\"\"\n",
    "r_crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the performance versus alpha for the RIDGE model\n",
    "using plot_param_val_for_size() for size indices 0, 3, and 7, \n",
    "for the optimized metric (use r_crossval.opt_metric)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE\n",
    "Use plot_param_val_surface_RL() to plot the surface of the training\n",
    "and validation set performance versus alpha and size in the X and Y axes,\n",
    "using the optimized metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 30\n",
    "# Angle the plot is viewed\n",
    "angle = 255\n",
    "\n",
    "# TODO: Plot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for LASSO\n",
    "\"\"\"\n",
    "l_crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Plot the performance versus alpha for the LASSO model\n",
    "using plot_param_val_for_size() for size indices 0, 3, and 7, \n",
    "for the optimized metric\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO\n",
    "Use plot_param_val_surface_RL() to plot the surface of the training\n",
    "and validation set performance versus alpha and size in the X and Y axes,\n",
    "using the optimized metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 30\n",
    "# Angle the plot is viewed\n",
    "angle = 255\n",
    "\n",
    "# TODO: Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "List the parameter sets explored for ELASTICNET\n",
    "\"\"\"\n",
    "crossval.paramsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET\n",
    "Use plot_param_val_surface_EN() to plot the surface of the training\n",
    "and validation set performance versus alpha and l1_ratio in the X \n",
    "and Y axes for the size indices of 0, 3, and 7, for crossval.opt_metric\n",
    "\"\"\"\n",
    "# Feel free to adjust these to understand the shape of the surface\n",
    "# Elevation of the plot\n",
    "elev = 25\n",
    "# Angle the plot is viewed\n",
    "angle = 280\n",
    "\n",
    "# TODO: Plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paired t-tests\n",
    "We can use paired t-tests to assess statistical significant differences between the mean test set performances of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PROVIDED\n",
    "Obtain all the results for all the models\n",
    "\"\"\"\n",
    "# LinearRegression\n",
    "lnr_all_results = lnr_crossval.results\n",
    "\n",
    "# RIDGE\n",
    "r_all_results = r_crossval.results\n",
    "\n",
    "# LASSO\n",
    "l_all_results = l_crossval.results\n",
    "\n",
    "# ELASTICNET\n",
    "all_results = crossval.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Complete the plotting code\n",
    "\n",
    "Plot distributions of the Validation and Test scores from the\n",
    "best parameter set for each base model for the corresponding \n",
    "size indices, [0, 3, 7]. The metric of interest is rmse_degs.\n",
    "These are the distribution of results from each rotation of \n",
    "the training set\n",
    "\"\"\"\n",
    "metric = 'rmse_degs'\n",
    "set_names = ['val', 'test']\n",
    "nbins = 11\n",
    "\n",
    "# Size indices\n",
    "size_indices = [0, 3, size_idx]\n",
    "\n",
    "for si in size_indices:\n",
    "    # Obtain the index of the best parameter set for the size\n",
    "    # RIDGE\n",
    "    r_bp_idx = r_crossval.best_param_inds[si]\n",
    "    # LASSO\n",
    "    l_bp_idx = l_crossval.best_param_inds[si]\n",
    "    # ELASTICNET\n",
    "    bp_idx = crossval.best_param_inds[si]\n",
    "\n",
    "    # Construct the figure\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15,7))\n",
    "    for i, set_name in enumerate(set_names):\n",
    "        title = '%s, Size %d' % (set_name, trainsizes[si])\n",
    "\n",
    "        # LINEAR\n",
    "        # Note: there's only 1 parameter set for the Linear model\n",
    "        lnr_res = lnr_all_results[0]['results'][si][set_name]\n",
    "        lnr_scores = np.mean(lnr_res[metric], axis=1)\n",
    "\n",
    "        # RIDGE\n",
    "        # Obtain results for the best parameter set for the size\n",
    "        ridge_res = r_all_results[r_bp_idx]['results'][si][set_name]\n",
    "        # Compute the mean of the outputs for each data set rotation\n",
    "        ridge_scores = np.mean(ridge_res[metric], axis=1)\n",
    "\n",
    "        # LASSO\n",
    "        lasso_res = l_all_results[l_bp_idx]['results'][si][set_name]\n",
    "        lasso_scores = np.mean(lasso_res[metric], axis=1)\n",
    "        \n",
    "        # ELASTICNET\n",
    "        res = # TODO\n",
    "        elastic_scores = # TODO\n",
    "        \n",
    "        # Determine the edges for the bins in the histograms\n",
    "        all_scores = np.concatenate((elastic_scores, ridge_scores, \n",
    "                                     lasso_scores, lnr_scores))\n",
    "        mn = np.min(all_scores)\n",
    "        mx = np.max(all_scores)\n",
    "        bins = np.linspace(mn, mx, nbins)\n",
    "\n",
    "        # Histograms\n",
    "        # TODO: include the hist of the elastic net scores \n",
    "        \n",
    "        \n",
    "        axs[0, i].hist(ridge_scores, bins=bins, alpha=.4)\n",
    "        axs[0, i].hist(lasso_scores, bins=bins, alpha=.4)\n",
    "        axs[0, i].hist(lnr_scores, bins=bins, alpha=.4)\n",
    "        axs[0, i].legend(['ElasticNet', 'Ridge', 'Lasso', 'Linear'])\n",
    "        axs[0, i].set(title=title, xlabel=metric)\n",
    "        \n",
    "        # Boxplots\n",
    "        axs[1, i].boxplot([elastic_scores, ridge_scores, lasso_scores, lnr_scores])\n",
    "        axs[1, i].set_xticklabels(['ElasticNet', 'Ridge', 'Lasso', 'Linear'])\n",
    "        axs[1, i].set(ylabel=metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Dependent Sample Paired t-test\n",
    "Two-sided t-test for the null hypothesis that mean of the distribution\n",
    "of differences between the two test performance distributions is zero \n",
    "\"\"\"\n",
    "print(\"Train Set Size\", trainsizes[size_idx])\n",
    "\n",
    "# LINEAR\n",
    "# Note: there's only 1 parameter set for the LinearRegression model\n",
    "lnr_res = lnr_crossval.results[0]['results'][size_idx]['test']\n",
    "lnr_test_res = np.mean(lnr_res[metric], axis=1)\n",
    "\n",
    "# RIDGE\n",
    "# Obtain index of best parameters for train size 8\n",
    "r_bp_idx = r_crossval.best_param_inds[size_idx]\n",
    "# Obtain all results for the best parameter set for train size 8\n",
    "ridge_res = r_all_results[r_bp_idx]['results'][size_idx]['test']\n",
    "# Compute the mean of the outputs for each data set rotation\n",
    "ridge_test_res = np.mean(ridge_res[metric], axis=1)\n",
    "\n",
    "# LASSO\n",
    "l_bp_idx = l_crossval.best_param_inds[size_idx]\n",
    "lasso_res = l_all_results[l_bp_idx]['results'][size_idx]['test']\n",
    "lasso_test_res = np.mean(lasso_res[metric], axis=1)\n",
    "\n",
    "# TODO: ELASTICNET\n",
    "bp_idx = # TODO\n",
    "net_res = # TODO\n",
    "elastic_test_res = # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET vs RIDGE\n",
    "Execute the paired t-test to determine whether to reject the null hypothesis \n",
    "(i.e. H0) with 95% confidence. H0 is that the mean of the distribution of the \n",
    "differences between test scores for the best ELASTICNET model and the best RIDGE \n",
    "is zero, when using a training size of 8 (i.e. the size at index 7 of the \n",
    "trainsizes list). Display the t-statistic, the p-value, and the mean of the \n",
    "differences (i.e. mean(elastic_test_res - ridge_test_res))\n",
    "\n",
    "Use stats.ttest_rel(). See the API reference above.\n",
    "Do the same for all the pairing of models\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET vs LASSO\n",
    "Execute the paired t-test\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "ELASTICNET vs LinearRegression\n",
    "Execute the paired t-test\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE vs LASSO\n",
    "Execute the paired t-test\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "RIDGE vs LinearRegression\n",
    "Execute the paired t-test\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "LASSO vs LinearRegression\n",
    "Execute the paired t-test\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION\n",
    "For each question write 1 to 2 paragraphs of discussion:\n",
    "\n",
    "1. Interpret the meaning of the t-test results using 95% confidence. Discuss the statistical meaning as well as the practical interpretation of the results in the context of the data set.\n",
    "\n",
    "\n",
    "2. For the Elastic Net Model, discuss the differences in the surfaces between the train sizes of 1, 4, and 8 folds, for the training and validation sets.\n",
    "\n",
    "\n",
    "3. For each of the train set sizes of 1, 4, and 8 folds, which model (Linear, Lasso, Ridge, or ElasticNet) and corresponding parameter set would you select and why? Specify which model and parameter set for each size. For each size, use plot_param_train_val() to view the train, val, and test sets of the chosen model(s). Remember, selections should be made based on the validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" TODO\n",
    "Discussion question 3 plots\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "232px",
    "width": "295px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
